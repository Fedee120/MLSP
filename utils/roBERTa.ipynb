{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff598a67b0244006aea404470ba16b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loque\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\huggingface_cache\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a00d05b2f44c0bafef89ee6f39dcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9fed3a299844198049aea2560b3d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadb4b411522480e80624940063acd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7dfeb0d0c946e5b9611ff1be51e0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6a1ace51824abdb9796c01b87ffc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de la oraciÃ³n: tensor([[-8.6743e-02,  9.0471e-02,  1.3302e-02, -1.0595e-01,  3.9593e-02,\n",
      "         -1.7688e-02, -4.2758e-02, -2.3705e-03,  3.3589e-02, -1.0090e-01,\n",
      "         -4.4411e-02, -1.7645e-02,  7.2555e-02, -4.6160e-02,  7.3803e-02,\n",
      "          2.9888e-02, -5.5485e-02, -2.1723e-02, -2.7182e-02, -3.6914e-02,\n",
      "         -1.0021e-01,  1.2966e-02, -9.6303e-03,  9.4810e-02, -2.5938e-02,\n",
      "          8.1167e-02,  8.1004e-02,  8.1191e-02, -6.7600e-02,  5.5690e-02,\n",
      "         -5.8468e-02, -7.9548e-02,  5.6678e-02,  1.0320e-02,  6.8747e-02,\n",
      "          1.3197e-01,  4.0268e-02, -2.0073e-02, -4.9358e-02,  9.8385e-03,\n",
      "         -7.8973e-03,  1.7162e-01, -1.2055e-02,  1.9111e-02,  6.3142e-02,\n",
      "         -3.0364e-03,  2.0137e-02, -6.0194e-03, -3.0871e-02,  2.8043e-02,\n",
      "          4.6304e-02,  6.4634e-02, -3.9413e-02,  3.3905e-02, -1.8681e-01,\n",
      "          4.6870e-02,  5.5762e-02,  7.0085e-02, -3.2284e-02, -8.9200e-02,\n",
      "         -3.3546e-02, -1.7492e-01, -9.4362e-02, -8.4185e-02,  7.9824e-02,\n",
      "         -6.4356e-02, -2.3966e-02, -2.3704e-03,  5.4052e-02,  4.2706e-02,\n",
      "          5.3084e-02, -7.1791e-02,  4.2474e-02, -6.8317e-02, -2.1265e-02,\n",
      "          1.7003e-03, -8.7166e-04,  5.8259e-01, -1.0546e-01, -2.6489e-03,\n",
      "          4.7962e-02, -8.8415e-02,  5.7433e-01,  3.7707e-02, -1.6393e-02,\n",
      "          2.2908e-02,  1.3285e-01,  2.0244e-02,  7.2175e-02,  1.5951e-02,\n",
      "         -5.1774e-03,  1.1182e-01, -2.1539e-02,  5.1326e-02,  6.6964e-02,\n",
      "          1.7041e-02, -1.0916e-02,  1.0652e-01, -6.6942e-02, -4.0852e-02,\n",
      "         -1.5964e-02, -3.3983e-02,  4.7101e-02,  5.3062e-02, -9.5862e-03,\n",
      "          9.7274e-03,  4.1852e-02, -5.5519e-02,  9.6809e-03, -7.3550e-02,\n",
      "         -1.0728e-02, -2.0072e-02,  1.3953e-02,  3.2299e-03,  1.9211e-02,\n",
      "         -8.4088e-03,  1.2214e-02,  3.7153e-02,  6.7847e-03,  3.7385e-02,\n",
      "          2.1776e-03,  1.3629e-01,  1.0260e-01, -9.1276e-02, -7.9824e-02,\n",
      "         -1.0284e-01, -4.9118e-02, -2.4890e-02,  2.6205e-02,  5.0407e-02,\n",
      "          1.2743e-02, -1.8179e-01,  9.2737e-03,  7.1937e-02,  2.0900e-02,\n",
      "          2.4236e-02,  6.4589e-02, -1.1895e-02,  2.2920e-02, -3.1835e-02,\n",
      "         -4.1262e-02,  7.0507e-02,  6.6173e-02,  5.8911e-02,  1.4351e-01,\n",
      "          1.0444e-01,  4.9884e-03, -8.7025e-02,  1.5014e-02, -1.4191e-02,\n",
      "          6.8622e-02, -6.6471e-02, -7.0430e-02,  7.2721e-03, -8.5029e-02,\n",
      "          4.8415e-01,  1.4333e-01,  1.2038e-01, -1.5991e-02, -2.5905e-02,\n",
      "          2.0410e-01,  1.6836e-02,  4.8408e-02, -5.0917e-02, -1.2027e-02,\n",
      "          1.8297e-02, -3.6015e-02, -4.5086e-02,  8.1114e-02,  6.3952e-03,\n",
      "          4.7197e-02,  3.7210e-02,  3.9942e-02, -2.4866e-02, -5.6492e-02,\n",
      "         -1.1750e-01,  7.4873e-02, -2.9555e-02, -4.8448e-02,  1.1334e-02,\n",
      "         -7.6473e-03,  6.8404e-02, -5.8331e-02, -1.4190e-04, -6.2722e-02,\n",
      "          3.8658e-02,  4.7389e-02,  8.7321e-03, -1.6752e-02,  5.8300e-02,\n",
      "          1.8411e-02,  9.6821e-03,  1.0701e-02,  7.1668e-03, -4.4908e-02,\n",
      "          1.5090e-01, -6.0346e-02, -9.8261e-02,  3.3121e-02, -9.0659e-02,\n",
      "          5.0987e-02, -1.0283e-02,  1.1567e-01, -7.7358e-02,  7.9105e-02,\n",
      "         -5.7630e-02,  1.4953e-03,  5.0021e-02,  3.3252e-02, -1.0287e-01,\n",
      "         -3.4301e-02,  9.8115e-02, -2.2453e-03,  1.1546e-01,  6.7875e-02,\n",
      "         -2.4123e-02,  4.9959e-02,  1.0019e-01,  4.4557e-02, -1.0391e-02,\n",
      "          6.3904e-02,  4.6639e-02,  1.8554e-02,  5.8790e-02,  1.5598e-02,\n",
      "          4.7685e-04,  3.9351e-02,  1.7820e-02,  3.4559e-02,  1.6993e-03,\n",
      "         -5.6440e-02,  6.1581e-02, -2.0883e-02,  1.9933e-02,  4.1554e-02,\n",
      "         -1.3436e-01, -6.7343e-02, -2.7221e-03, -5.5864e-02,  2.1717e-02,\n",
      "         -6.8222e-02,  8.5155e-02,  7.7371e-02,  4.5317e-02, -2.3328e-02,\n",
      "          9.4404e-02,  1.8099e-02,  1.3443e-01, -3.5622e-02,  5.6076e-03,\n",
      "         -3.2309e-02, -7.1218e-02,  1.3196e-02, -1.5551e-02,  7.6709e-02,\n",
      "         -5.3467e-02, -7.7599e-02, -2.5763e-02, -5.2336e-02, -7.7012e-02,\n",
      "         -5.6431e-02, -1.8184e-02, -3.5745e-02, -2.6823e-02, -8.3007e-02,\n",
      "         -8.2588e-03, -5.7734e-02, -1.2789e-02,  2.3955e-02, -5.7827e-02,\n",
      "         -4.3641e-02, -7.6102e-02,  2.8734e-02, -5.5532e-02,  6.5057e-02,\n",
      "         -1.8184e-02,  3.2177e-02,  3.3695e-02, -6.3900e-02,  3.2412e-03,\n",
      "          1.3327e-02, -8.0932e-02, -7.2061e-02, -2.2343e-02,  4.3556e-02,\n",
      "          2.7921e-02, -1.1189e-01, -2.2369e-03,  2.1895e-02,  3.3051e-02,\n",
      "          7.1640e-02,  6.4862e-02, -4.5362e-02,  7.1810e-03,  6.7568e-03,\n",
      "          2.8241e-02,  1.1763e-01,  2.3649e-03,  2.0152e-02,  4.5825e-02,\n",
      "         -7.6199e-02, -6.2303e-02, -7.4942e-02, -6.0138e-04, -1.3798e-02,\n",
      "         -4.5531e-02, -4.1855e-03, -1.4137e-02,  8.3296e-03, -3.6659e-02,\n",
      "         -8.4431e-02, -4.6193e-02, -1.1460e-01,  1.2373e-01,  3.3450e-02,\n",
      "          5.2874e-02,  1.1699e-01, -4.8592e-02,  3.2756e-03,  9.1486e-03,\n",
      "         -1.5595e-02,  1.6432e-02,  8.7169e-02, -3.0965e-02,  1.7055e-02,\n",
      "          3.7752e-02, -1.2711e-02, -4.4711e-03,  3.1508e-02,  3.7556e-01,\n",
      "         -4.3821e-01,  6.0415e-02,  5.3841e-02, -1.6204e-02,  6.8832e-02,\n",
      "         -4.0509e-02,  2.5890e-02,  6.8478e-02,  1.2236e-01,  1.2096e-01,\n",
      "          2.7753e-03,  4.6244e-02, -6.1174e-02,  2.0870e-02,  4.1483e-02,\n",
      "          3.9333e-02,  3.4249e-02, -7.2197e-02,  1.1797e-02,  4.6559e-02,\n",
      "         -4.9932e-02, -3.5493e-02, -5.2618e-03, -1.2755e-01,  1.0938e-02,\n",
      "          7.1237e-02,  2.6775e-02, -1.6675e-02,  5.3056e-02, -1.7099e-02,\n",
      "          5.7736e-02, -3.6445e-02,  1.0127e-02, -6.9665e-02,  2.2873e-02,\n",
      "         -1.4900e-02, -3.9407e-02,  8.7542e-02,  7.8053e-03,  1.6078e-02,\n",
      "          7.2788e-02, -1.0226e-01, -1.8198e-02,  5.3635e-02, -5.4739e-02,\n",
      "          5.3820e-02,  6.6781e-02,  5.3485e-03,  2.6720e-02,  1.3598e-01,\n",
      "         -3.6011e-02,  2.6903e-02, -6.1963e-02,  3.8163e-02,  1.2751e-01,\n",
      "         -5.6785e-02,  2.9698e-02,  3.3415e-03,  1.1842e-02,  5.0823e-02,\n",
      "         -7.7384e-03, -3.6000e-02, -7.7595e-02, -7.9368e-02,  9.3841e-02,\n",
      "          2.2566e-02, -2.7545e-02, -1.2230e-01,  3.7427e-02, -2.3557e-02,\n",
      "          3.7160e-03,  5.8824e-02, -1.1056e-01,  2.3237e-02, -3.5951e-02,\n",
      "          4.7256e-02, -5.9930e-03, -4.3082e-02,  4.8445e-03, -8.8143e-02,\n",
      "         -3.2571e-03,  5.6883e-02,  7.1935e-02, -7.3077e-02, -1.0337e-02,\n",
      "         -6.1761e-02,  2.0064e-02, -1.3042e-02,  2.8062e-02, -3.7818e-02,\n",
      "         -9.2844e-02,  7.6943e-02,  1.5383e-02, -3.6681e-03, -6.0810e-02,\n",
      "          1.7291e-02, -5.2621e-03, -9.6783e-02, -7.4436e-02, -3.8297e-02,\n",
      "         -8.4023e-02,  7.7395e-02, -6.6495e-02, -5.2418e-02, -4.0357e-02,\n",
      "         -1.2484e-02, -9.0853e-03,  8.8095e-03, -3.9228e-02, -6.3118e-02,\n",
      "         -1.3712e-02, -3.8378e-02,  6.9894e-02, -4.1108e-02,  3.5698e-02,\n",
      "         -1.1856e-02,  5.6761e-02,  3.1018e-02, -5.1895e-02,  2.6637e-02,\n",
      "          4.4506e-02, -2.5184e-02, -3.4316e-02, -3.9187e-01,  1.6507e-02,\n",
      "         -2.3843e-02, -1.4641e-02,  3.4708e-02, -1.1836e-01, -2.1476e-02,\n",
      "          3.0096e-02,  1.9386e-02,  5.1916e-02, -5.2629e-02,  5.3614e-02,\n",
      "          1.2981e-02, -1.2577e-01,  2.6440e-02, -3.0029e-02, -6.7605e-02,\n",
      "          5.3799e-02, -3.2834e-02, -3.5096e-03, -9.1976e-02,  2.5720e-02,\n",
      "         -7.5433e-03,  7.5056e-02,  1.6252e-02, -2.5973e-02, -3.4907e-02,\n",
      "         -1.0997e-01,  2.5547e-02,  4.9485e-02,  3.0356e-02, -8.8491e-02,\n",
      "         -3.8394e-02, -1.3596e-02,  1.0127e-01,  9.7609e-02, -1.1586e-02,\n",
      "         -3.1623e-02, -1.4896e-02,  7.4673e-02,  5.8792e-02,  2.0987e-01,\n",
      "         -2.3352e-02,  1.8499e-01,  8.0364e-03,  1.4472e-01,  2.0066e-02,\n",
      "         -2.0418e-02, -6.2267e-02, -2.8394e-02,  2.9958e-02, -2.7348e-02,\n",
      "          1.8605e-02, -6.1872e-02, -4.7208e-02, -4.7673e-04,  3.5045e-02,\n",
      "         -2.0042e-03, -1.2379e-02,  1.2889e-01, -5.6674e-03,  5.7976e-02,\n",
      "         -2.3726e-03, -3.2176e-02,  2.2825e-03, -5.0932e-02, -1.4484e-02,\n",
      "         -2.8643e-02, -1.0998e-02, -6.7145e-03, -2.8820e-02, -5.0850e-02,\n",
      "          3.0008e-02,  9.0867e-04, -2.8691e-03,  1.2912e-01, -2.1780e-02,\n",
      "          7.1706e-02, -1.6418e-02,  9.0337e-02,  3.6220e-02,  4.2136e-02,\n",
      "          6.9658e-02,  8.3700e-02,  3.0263e-02, -4.8790e-02, -1.1796e-02,\n",
      "         -4.3879e-02,  1.5296e-02,  1.2515e-01, -1.4693e-02,  6.3432e-02,\n",
      "          4.6070e-02,  1.8349e-03, -6.7392e-02,  4.7925e-02, -6.1569e-03,\n",
      "          4.6544e-02, -6.3297e-01, -1.7998e-02,  6.0228e-02, -6.1232e-02,\n",
      "          3.7940e-02,  8.0922e-02,  1.0205e-01, -6.9558e-02, -7.6348e-02,\n",
      "         -4.6737e-02,  3.9310e-02,  2.6103e-02,  1.1121e-01, -5.0523e-02,\n",
      "          8.3239e-03,  6.4006e-02, -2.5629e-02, -1.9746e-03,  2.3842e-02,\n",
      "         -1.8833e-01, -3.5848e-02, -9.9692e-02,  6.4010e-02,  6.0715e-03,\n",
      "          2.6989e-02,  9.6472e-02, -4.8633e-02,  3.0026e-02,  4.0709e-02,\n",
      "          3.3020e-02,  4.7209e-02,  7.9793e-02, -2.1064e-02,  6.7029e-02,\n",
      "          3.6862e-02,  2.1207e-02,  3.3444e-02,  1.1688e+01, -3.3950e-02,\n",
      "          6.2024e-02, -2.2192e-02,  8.9378e-02, -9.0813e-02,  3.3272e-02,\n",
      "         -1.3194e-01, -2.4420e-02,  1.0027e-01,  1.8954e-03, -5.5741e-02,\n",
      "         -6.6146e-02, -7.9206e-02,  5.1552e-02,  6.5806e-03, -5.3897e-02,\n",
      "         -3.3205e-02,  5.1019e-02, -7.2768e-02, -2.9524e-02,  3.1030e-02,\n",
      "          4.5191e-02,  7.8183e-03, -7.7772e-02, -2.4333e-02,  9.7515e-02,\n",
      "         -4.3550e-02, -3.2142e-03,  1.1714e-02, -1.3523e-02,  1.0736e-02,\n",
      "          2.9821e-02,  5.3838e-03,  1.3487e-01,  9.5326e-03,  1.6973e-02,\n",
      "          5.9456e-02,  6.8511e-03,  3.9401e-02,  2.0757e-02, -1.4802e-02,\n",
      "          5.3943e-02,  3.3357e-03,  8.2558e-02,  5.0526e-02, -1.5916e-02,\n",
      "          1.0252e-01,  1.7452e-02,  3.0724e-02,  1.1270e-01, -5.8599e-02,\n",
      "          1.1657e-01,  2.8744e-03, -1.9818e-02,  3.5658e-02,  1.9350e-02,\n",
      "          2.2837e-03,  5.1655e-02,  4.5689e-02, -8.0219e-02,  7.6608e-02,\n",
      "         -1.0359e-02,  9.4136e-03,  1.6960e-02,  1.5585e-01,  1.2570e-01,\n",
      "          1.0169e-01, -9.1311e-02, -5.2549e-02,  1.8294e-02, -4.6609e-02,\n",
      "         -9.7340e-02,  9.2756e-03,  9.1867e-02, -3.9546e-02, -5.0252e-02,\n",
      "          2.5232e-02, -5.0176e-02, -4.5388e-02,  7.4673e-04,  1.1431e-02,\n",
      "          7.0852e-02, -1.1171e-02,  1.4328e-01,  7.2943e-02,  4.3730e-02,\n",
      "          4.7126e-02, -2.1214e-02, -3.7605e-02, -1.0752e-02,  1.4330e-02,\n",
      "         -9.5567e-04, -7.0456e-02, -1.7287e-02, -6.2811e-02,  2.4195e-02,\n",
      "         -1.3933e-01,  2.6548e-03,  6.0007e-02, -1.0736e-01,  3.3857e-02,\n",
      "         -8.7799e-03,  5.2284e-02,  4.4929e-02,  4.3290e-02, -9.9814e-02,\n",
      "         -3.0825e-02,  1.8337e-02, -9.4105e-03, -3.1660e-02,  6.6285e-03,\n",
      "          8.0725e-02, -5.7424e-02,  3.3422e-04,  5.2773e-02,  4.2248e-02,\n",
      "         -1.3652e-02,  5.9521e-02,  1.0129e-01, -1.0252e-01, -3.1620e-02,\n",
      "         -3.0927e-02, -5.7649e-02, -3.2964e-02, -4.4436e-02,  1.1286e-02,\n",
      "          1.1290e-02, -9.7262e-03,  3.5122e-02,  1.9145e-02,  3.9285e-02,\n",
      "          2.4979e-02, -5.6798e-02,  7.2913e-02, -6.7388e-02, -7.7330e-02,\n",
      "          6.2795e-02,  6.4434e-02,  7.3127e-02,  3.7289e-02, -2.3990e-02,\n",
      "         -1.1057e-01, -8.7222e-02,  4.9239e-02,  6.7422e-02,  5.2799e-02,\n",
      "          7.7512e-02,  7.4028e-02, -1.2066e-01,  1.2572e-02,  1.5012e-02,\n",
      "          5.2393e-02,  1.4407e-02,  7.1810e-03, -2.4598e-02, -2.7794e-02,\n",
      "          8.4707e-02, -3.1796e-02,  1.6741e-02,  6.1139e-02,  3.0692e-02,\n",
      "          8.7058e-02,  5.8987e-02, -7.9977e-03,  4.7452e-02, -1.7074e-02,\n",
      "          4.8237e-02,  2.7334e-03, -2.1023e-02, -7.5208e-04, -1.3585e-02,\n",
      "         -1.1437e-01, -1.0344e-01,  1.5145e-02,  1.4822e-01,  4.3537e-03,\n",
      "         -4.8190e-02, -1.9033e-02, -4.6460e-02]])\n",
      "Embeddings de las palabras: tensor([[[-0.0867,  0.0905,  0.0133,  ..., -0.0482, -0.0190, -0.0465],\n",
      "         [ 0.0652, -0.0678, -0.1393,  ...,  0.0322,  0.0991,  0.1104],\n",
      "         [-0.0032,  0.2392, -0.0245,  ..., -0.3839,  0.0199,  0.1572],\n",
      "         ...,\n",
      "         [-0.0129,  0.2340, -0.0339,  ...,  0.0248,  0.1979,  0.2229],\n",
      "         [-0.0739,  0.0814, -0.0025,  ..., -0.0717, -0.0166, -0.0734],\n",
      "         [-0.0039,  0.0613, -0.0262,  ...,  0.1493,  0.0737,  0.1269]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Carga el tokenizador y el modelo\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "#model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# OraciÃ³n de ejemplo\n",
    "sentence = \"Hola, esto es una prueba con BERT.\"\n",
    "\n",
    "# Tokeniza la oraciÃ³n\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Obtiene los embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Los embeddings de la Ãºltima capa oculta serÃ­an\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Embedding del [CLS] token que representa la oraciÃ³n entera\n",
    "sentence_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "# Embeddings de cada token/palabra en la oraciÃ³n\n",
    "word_embeddings = last_hidden_states\n",
    "\n",
    "print(\"Embedding de la oraciÃ³n:\", sentence_embedding)\n",
    "print(\"Embeddings de las palabras:\", word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json\n",
    "# Supongamos que tus datos estÃ¡n en un archivo JSON llamado 'converted_data.jsonl'\n",
    "with open('../scripts/converted_data.jsonl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Convertir los datos en un DataFrame de pandas\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_word</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Behold, there came up out of the river seven c...</td>\n",
       "      <td>river</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am a fellow bondservant with you and with yo...</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The man, the lord of the land, said to us, 'By...</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shimei had sixteen sons and six daughters; but...</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"He has put my brothers far from me.</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.263889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence target_word  complexity\n",
       "0  Behold, there came up out of the river seven c...       river    0.000000\n",
       "1  I am a fellow bondservant with you and with yo...    brothers    0.000000\n",
       "2  The man, the lord of the land, said to us, 'By...    brothers    0.050000\n",
       "3  Shimei had sixteen sons and six daughters; but...    brothers    0.150000\n",
       "4               \"He has put my brothers far from me.    brothers    0.263889"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename columns\n",
    "df.rename(columns={'sentence': 'sentence', 'token': 'target_word', 'complexity': 'complexity'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353031ac3d194f16b5163fa0bb5e3ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/6129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9798e8648f314d3f97019fb22278307a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/6129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d537819e944a4406b30bcf16bc3d7d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bc818d240c478fbc358069248c544e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suponiendo que df es tu DataFrame que incluye las columnas 'sentence', 'target_word' y 'complexity'\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_embeddings(text_list, tokenizer, model, device):\n",
    "    \"\"\"Obtiene embeddings [CLS] para una lista de textos, con barra de progreso.\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for text in tqdm(text_list, desc=\"Processing\", leave=True):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded_input)\n",
    "        embeddings.append(output.last_hidden_state[:, 0, :].squeeze().cpu())\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "\n",
    "def process_dataframe(df, tokenizer, model, device):\n",
    "    \"\"\"Procesa un dataframe para obtener embeddings y complejidades.\"\"\"\n",
    "    sentence_embeddings = get_embeddings(df['sentence'].tolist(), tokenizer, model, device)\n",
    "    word_embeddings = get_embeddings(df['target_word'].tolist(), tokenizer, model, device)\n",
    "    complexities = torch.tensor(df['complexity'].values, dtype=torch.float).unsqueeze(1)\n",
    "    return torch.cat((sentence_embeddings, word_embeddings, complexities), dim=1)\n",
    "\n",
    "# Procesar los conjuntos de entrenamiento y prueba\n",
    "train_data = process_dataframe(train_df, tokenizer, model, device)\n",
    "test_data = process_dataframe(test_df, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6129, 1537]), torch.Size([1533, 1537]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entradas de entrenamiento (todas las columnas excepto la Ãºltima)\n",
    "train_features = train_data[:, :-1]\n",
    "# Objetivos de entrenamiento (Ãºltima columna)\n",
    "train_targets = train_data[:, -1]\n",
    "\n",
    "# Entradas de prueba\n",
    "test_features = test_data[:, :-1]\n",
    "# Objetivos de prueba\n",
    "test_targets = test_data[:, -1]\n",
    "\n",
    "# AsegÃºrate de que los tensores estÃ©n en la CPU para convertirlos a arrays de NumPy\n",
    "train_features_np = train_features.numpy()\n",
    "train_targets_np = train_targets.numpy()\n",
    "test_features_np = test_features.numpy()\n",
    "test_targets_np = test_targets.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\loque\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 256)               393472    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 426497 (1.63 MB)\n",
      "Trainable params: 426497 (1.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.15),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=rmse,\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.0869 - mean_absolute_error: 0.0678 - val_loss: 0.1112 - val_mean_absolute_error: 0.0848\n",
      "Epoch 2/5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.0871 - mean_absolute_error: 0.0677 - val_loss: 0.1091 - val_mean_absolute_error: 0.0836\n",
      "Epoch 3/5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.0872 - mean_absolute_error: 0.0677 - val_loss: 0.1079 - val_mean_absolute_error: 0.0834\n",
      "Epoch 4/5\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.0888 - mean_absolute_error: 0.0693 - val_loss: 0.1100 - val_mean_absolute_error: 0.0847\n",
      "Epoch 5/5\n",
      "154/154 [==============================] - 1s 3ms/step - loss: 0.0868 - mean_absolute_error: 0.0675 - val_loss: 0.1032 - val_mean_absolute_error: 0.0799\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "history = model.fit(train_features_np, train_targets_np,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=5,\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 - 0s - loss: 0.1020 - mean_absolute_error: 0.0782 - 78ms/epoch - 2ms/step\n",
      "Test RMSE: 0.10195381194353104, Test MAE: 0.07821858674287796\n"
     ]
    }
   ],
   "source": [
    "# EvaluaciÃ³n del modelo\n",
    "test_loss, test_mae = model.evaluate(test_features_np, test_targets_np, verbose=2)\n",
    "print(f\"Test RMSE: {test_loss}, Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelos/roberta.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
