{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spoturno/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de la oraci√≥n: tensor([[-8.6743e-02,  9.0471e-02,  1.3302e-02, -1.0595e-01,  3.9593e-02,\n",
      "         -1.7688e-02, -4.2758e-02, -2.3704e-03,  3.3589e-02, -1.0090e-01,\n",
      "         -4.4411e-02, -1.7645e-02,  7.2555e-02, -4.6160e-02,  7.3803e-02,\n",
      "          2.9888e-02, -5.5485e-02, -2.1723e-02, -2.7182e-02, -3.6914e-02,\n",
      "         -1.0021e-01,  1.2966e-02, -9.6302e-03,  9.4810e-02, -2.5938e-02,\n",
      "          8.1167e-02,  8.1004e-02,  8.1191e-02, -6.7600e-02,  5.5690e-02,\n",
      "         -5.8468e-02, -7.9548e-02,  5.6678e-02,  1.0321e-02,  6.8747e-02,\n",
      "          1.3197e-01,  4.0268e-02, -2.0073e-02, -4.9358e-02,  9.8386e-03,\n",
      "         -7.8971e-03,  1.7162e-01, -1.2055e-02,  1.9111e-02,  6.3142e-02,\n",
      "         -3.0365e-03,  2.0137e-02, -6.0194e-03, -3.0871e-02,  2.8043e-02,\n",
      "          4.6304e-02,  6.4634e-02, -3.9413e-02,  3.3905e-02, -1.8681e-01,\n",
      "          4.6870e-02,  5.5762e-02,  7.0085e-02, -3.2284e-02, -8.9200e-02,\n",
      "         -3.3546e-02, -1.7492e-01, -9.4362e-02, -8.4185e-02,  7.9824e-02,\n",
      "         -6.4356e-02, -2.3966e-02, -2.3705e-03,  5.4052e-02,  4.2706e-02,\n",
      "          5.3083e-02, -7.1791e-02,  4.2474e-02, -6.8317e-02, -2.1265e-02,\n",
      "          1.7003e-03, -8.7144e-04,  5.8259e-01, -1.0546e-01, -2.6488e-03,\n",
      "          4.7962e-02, -8.8415e-02,  5.7433e-01,  3.7707e-02, -1.6393e-02,\n",
      "          2.2908e-02,  1.3285e-01,  2.0244e-02,  7.2175e-02,  1.5951e-02,\n",
      "         -5.1774e-03,  1.1182e-01, -2.1540e-02,  5.1325e-02,  6.6964e-02,\n",
      "          1.7041e-02, -1.0916e-02,  1.0652e-01, -6.6942e-02, -4.0852e-02,\n",
      "         -1.5964e-02, -3.3983e-02,  4.7101e-02,  5.3062e-02, -9.5862e-03,\n",
      "          9.7275e-03,  4.1852e-02, -5.5519e-02,  9.6810e-03, -7.3549e-02,\n",
      "         -1.0728e-02, -2.0072e-02,  1.3953e-02,  3.2299e-03,  1.9211e-02,\n",
      "         -8.4088e-03,  1.2214e-02,  3.7154e-02,  6.7844e-03,  3.7385e-02,\n",
      "          2.1775e-03,  1.3629e-01,  1.0260e-01, -9.1276e-02, -7.9824e-02,\n",
      "         -1.0284e-01, -4.9118e-02, -2.4890e-02,  2.6205e-02,  5.0407e-02,\n",
      "          1.2743e-02, -1.8179e-01,  9.2737e-03,  7.1937e-02,  2.0900e-02,\n",
      "          2.4236e-02,  6.4589e-02, -1.1895e-02,  2.2920e-02, -3.1835e-02,\n",
      "         -4.1262e-02,  7.0507e-02,  6.6173e-02,  5.8911e-02,  1.4351e-01,\n",
      "          1.0444e-01,  4.9884e-03, -8.7025e-02,  1.5014e-02, -1.4191e-02,\n",
      "          6.8622e-02, -6.6471e-02, -7.0429e-02,  7.2723e-03, -8.5029e-02,\n",
      "          4.8415e-01,  1.4333e-01,  1.2038e-01, -1.5992e-02, -2.5905e-02,\n",
      "          2.0410e-01,  1.6836e-02,  4.8408e-02, -5.0916e-02, -1.2027e-02,\n",
      "          1.8297e-02, -3.6015e-02, -4.5086e-02,  8.1114e-02,  6.3954e-03,\n",
      "          4.7197e-02,  3.7210e-02,  3.9942e-02, -2.4866e-02, -5.6492e-02,\n",
      "         -1.1750e-01,  7.4873e-02, -2.9555e-02, -4.8449e-02,  1.1334e-02,\n",
      "         -7.6471e-03,  6.8405e-02, -5.8331e-02, -1.4195e-04, -6.2722e-02,\n",
      "          3.8658e-02,  4.7389e-02,  8.7322e-03, -1.6752e-02,  5.8300e-02,\n",
      "          1.8411e-02,  9.6821e-03,  1.0701e-02,  7.1668e-03, -4.4909e-02,\n",
      "          1.5090e-01, -6.0346e-02, -9.8261e-02,  3.3121e-02, -9.0658e-02,\n",
      "          5.0987e-02, -1.0283e-02,  1.1567e-01, -7.7358e-02,  7.9104e-02,\n",
      "         -5.7630e-02,  1.4953e-03,  5.0021e-02,  3.3252e-02, -1.0287e-01,\n",
      "         -3.4301e-02,  9.8115e-02, -2.2453e-03,  1.1546e-01,  6.7875e-02,\n",
      "         -2.4123e-02,  4.9959e-02,  1.0019e-01,  4.4557e-02, -1.0391e-02,\n",
      "          6.3904e-02,  4.6639e-02,  1.8554e-02,  5.8790e-02,  1.5598e-02,\n",
      "          4.7695e-04,  3.9351e-02,  1.7820e-02,  3.4559e-02,  1.6993e-03,\n",
      "         -5.6440e-02,  6.1581e-02, -2.0883e-02,  1.9933e-02,  4.1554e-02,\n",
      "         -1.3436e-01, -6.7343e-02, -2.7223e-03, -5.5864e-02,  2.1717e-02,\n",
      "         -6.8222e-02,  8.5155e-02,  7.7371e-02,  4.5317e-02, -2.3328e-02,\n",
      "          9.4403e-02,  1.8099e-02,  1.3443e-01, -3.5622e-02,  5.6076e-03,\n",
      "         -3.2309e-02, -7.1218e-02,  1.3196e-02, -1.5551e-02,  7.6710e-02,\n",
      "         -5.3467e-02, -7.7599e-02, -2.5763e-02, -5.2336e-02, -7.7012e-02,\n",
      "         -5.6431e-02, -1.8184e-02, -3.5745e-02, -2.6823e-02, -8.3008e-02,\n",
      "         -8.2588e-03, -5.7734e-02, -1.2789e-02,  2.3955e-02, -5.7827e-02,\n",
      "         -4.3641e-02, -7.6102e-02,  2.8734e-02, -5.5532e-02,  6.5056e-02,\n",
      "         -1.8184e-02,  3.2177e-02,  3.3695e-02, -6.3900e-02,  3.2411e-03,\n",
      "          1.3327e-02, -8.0932e-02, -7.2062e-02, -2.2342e-02,  4.3556e-02,\n",
      "          2.7921e-02, -1.1189e-01, -2.2369e-03,  2.1895e-02,  3.3051e-02,\n",
      "          7.1640e-02,  6.4862e-02, -4.5362e-02,  7.1812e-03,  6.7567e-03,\n",
      "          2.8241e-02,  1.1763e-01,  2.3648e-03,  2.0152e-02,  4.5825e-02,\n",
      "         -7.6199e-02, -6.2303e-02, -7.4943e-02, -6.0150e-04, -1.3798e-02,\n",
      "         -4.5531e-02, -4.1853e-03, -1.4137e-02,  8.3297e-03, -3.6659e-02,\n",
      "         -8.4431e-02, -4.6193e-02, -1.1460e-01,  1.2373e-01,  3.3450e-02,\n",
      "          5.2874e-02,  1.1699e-01, -4.8592e-02,  3.2757e-03,  9.1487e-03,\n",
      "         -1.5595e-02,  1.6432e-02,  8.7169e-02, -3.0965e-02,  1.7055e-02,\n",
      "          3.7752e-02, -1.2711e-02, -4.4712e-03,  3.1508e-02,  3.7556e-01,\n",
      "         -4.3821e-01,  6.0415e-02,  5.3841e-02, -1.6204e-02,  6.8832e-02,\n",
      "         -4.0509e-02,  2.5891e-02,  6.8478e-02,  1.2236e-01,  1.2096e-01,\n",
      "          2.7753e-03,  4.6244e-02, -6.1174e-02,  2.0870e-02,  4.1483e-02,\n",
      "          3.9333e-02,  3.4249e-02, -7.2197e-02,  1.1797e-02,  4.6559e-02,\n",
      "         -4.9932e-02, -3.5493e-02, -5.2618e-03, -1.2755e-01,  1.0938e-02,\n",
      "          7.1237e-02,  2.6775e-02, -1.6675e-02,  5.3055e-02, -1.7099e-02,\n",
      "          5.7736e-02, -3.6445e-02,  1.0127e-02, -6.9665e-02,  2.2873e-02,\n",
      "         -1.4900e-02, -3.9407e-02,  8.7542e-02,  7.8054e-03,  1.6078e-02,\n",
      "          7.2788e-02, -1.0226e-01, -1.8198e-02,  5.3635e-02, -5.4738e-02,\n",
      "          5.3820e-02,  6.6781e-02,  5.3486e-03,  2.6720e-02,  1.3598e-01,\n",
      "         -3.6011e-02,  2.6903e-02, -6.1963e-02,  3.8163e-02,  1.2751e-01,\n",
      "         -5.6785e-02,  2.9698e-02,  3.3416e-03,  1.1842e-02,  5.0823e-02,\n",
      "         -7.7384e-03, -3.6000e-02, -7.7595e-02, -7.9368e-02,  9.3840e-02,\n",
      "          2.2566e-02, -2.7545e-02, -1.2230e-01,  3.7427e-02, -2.3556e-02,\n",
      "          3.7161e-03,  5.8824e-02, -1.1056e-01,  2.3238e-02, -3.5951e-02,\n",
      "          4.7256e-02, -5.9929e-03, -4.3081e-02,  4.8444e-03, -8.8143e-02,\n",
      "         -3.2573e-03,  5.6883e-02,  7.1935e-02, -7.3077e-02, -1.0337e-02,\n",
      "         -6.1761e-02,  2.0064e-02, -1.3042e-02,  2.8062e-02, -3.7818e-02,\n",
      "         -9.2844e-02,  7.6943e-02,  1.5382e-02, -3.6681e-03, -6.0810e-02,\n",
      "          1.7291e-02, -5.2622e-03, -9.6783e-02, -7.4436e-02, -3.8297e-02,\n",
      "         -8.4023e-02,  7.7395e-02, -6.6495e-02, -5.2418e-02, -4.0357e-02,\n",
      "         -1.2484e-02, -9.0853e-03,  8.8094e-03, -3.9228e-02, -6.3117e-02,\n",
      "         -1.3712e-02, -3.8378e-02,  6.9894e-02, -4.1108e-02,  3.5698e-02,\n",
      "         -1.1856e-02,  5.6761e-02,  3.1018e-02, -5.1896e-02,  2.6637e-02,\n",
      "          4.4505e-02, -2.5184e-02, -3.4316e-02, -3.9187e-01,  1.6507e-02,\n",
      "         -2.3843e-02, -1.4641e-02,  3.4708e-02, -1.1836e-01, -2.1476e-02,\n",
      "          3.0096e-02,  1.9386e-02,  5.1916e-02, -5.2630e-02,  5.3614e-02,\n",
      "          1.2982e-02, -1.2577e-01,  2.6440e-02, -3.0028e-02, -6.7605e-02,\n",
      "          5.3799e-02, -3.2834e-02, -3.5097e-03, -9.1976e-02,  2.5720e-02,\n",
      "         -7.5433e-03,  7.5056e-02,  1.6252e-02, -2.5973e-02, -3.4907e-02,\n",
      "         -1.0997e-01,  2.5547e-02,  4.9485e-02,  3.0355e-02, -8.8491e-02,\n",
      "         -3.8394e-02, -1.3596e-02,  1.0127e-01,  9.7609e-02, -1.1586e-02,\n",
      "         -3.1623e-02, -1.4896e-02,  7.4673e-02,  5.8792e-02,  2.0987e-01,\n",
      "         -2.3352e-02,  1.8499e-01,  8.0364e-03,  1.4472e-01,  2.0066e-02,\n",
      "         -2.0418e-02, -6.2267e-02, -2.8395e-02,  2.9958e-02, -2.7348e-02,\n",
      "          1.8605e-02, -6.1872e-02, -4.7208e-02, -4.7697e-04,  3.5045e-02,\n",
      "         -2.0043e-03, -1.2379e-02,  1.2889e-01, -5.6674e-03,  5.7976e-02,\n",
      "         -2.3725e-03, -3.2176e-02,  2.2824e-03, -5.0932e-02, -1.4484e-02,\n",
      "         -2.8643e-02, -1.0998e-02, -6.7145e-03, -2.8820e-02, -5.0850e-02,\n",
      "          3.0008e-02,  9.0869e-04, -2.8689e-03,  1.2912e-01, -2.1780e-02,\n",
      "          7.1706e-02, -1.6418e-02,  9.0337e-02,  3.6220e-02,  4.2136e-02,\n",
      "          6.9658e-02,  8.3700e-02,  3.0262e-02, -4.8790e-02, -1.1796e-02,\n",
      "         -4.3879e-02,  1.5296e-02,  1.2515e-01, -1.4693e-02,  6.3432e-02,\n",
      "          4.6070e-02,  1.8349e-03, -6.7392e-02,  4.7925e-02, -6.1569e-03,\n",
      "          4.6544e-02, -6.3297e-01, -1.7998e-02,  6.0229e-02, -6.1232e-02,\n",
      "          3.7940e-02,  8.0922e-02,  1.0205e-01, -6.9558e-02, -7.6348e-02,\n",
      "         -4.6738e-02,  3.9310e-02,  2.6103e-02,  1.1121e-01, -5.0523e-02,\n",
      "          8.3239e-03,  6.4006e-02, -2.5629e-02, -1.9748e-03,  2.3842e-02,\n",
      "         -1.8833e-01, -3.5848e-02, -9.9692e-02,  6.4010e-02,  6.0714e-03,\n",
      "          2.6989e-02,  9.6472e-02, -4.8633e-02,  3.0026e-02,  4.0709e-02,\n",
      "          3.3020e-02,  4.7209e-02,  7.9793e-02, -2.1064e-02,  6.7029e-02,\n",
      "          3.6862e-02,  2.1208e-02,  3.3444e-02,  1.1688e+01, -3.3950e-02,\n",
      "          6.2024e-02, -2.2192e-02,  8.9378e-02, -9.0813e-02,  3.3272e-02,\n",
      "         -1.3194e-01, -2.4420e-02,  1.0027e-01,  1.8953e-03, -5.5741e-02,\n",
      "         -6.6146e-02, -7.9206e-02,  5.1552e-02,  6.5808e-03, -5.3897e-02,\n",
      "         -3.3205e-02,  5.1019e-02, -7.2768e-02, -2.9524e-02,  3.1030e-02,\n",
      "          4.5191e-02,  7.8182e-03, -7.7772e-02, -2.4332e-02,  9.7515e-02,\n",
      "         -4.3550e-02, -3.2143e-03,  1.1714e-02, -1.3523e-02,  1.0736e-02,\n",
      "          2.9821e-02,  5.3837e-03,  1.3487e-01,  9.5325e-03,  1.6973e-02,\n",
      "          5.9457e-02,  6.8512e-03,  3.9401e-02,  2.0757e-02, -1.4802e-02,\n",
      "          5.3943e-02,  3.3357e-03,  8.2559e-02,  5.0526e-02, -1.5916e-02,\n",
      "          1.0252e-01,  1.7452e-02,  3.0724e-02,  1.1270e-01, -5.8599e-02,\n",
      "          1.1657e-01,  2.8743e-03, -1.9818e-02,  3.5658e-02,  1.9350e-02,\n",
      "          2.2838e-03,  5.1655e-02,  4.5689e-02, -8.0219e-02,  7.6608e-02,\n",
      "         -1.0359e-02,  9.4137e-03,  1.6960e-02,  1.5585e-01,  1.2570e-01,\n",
      "          1.0169e-01, -9.1311e-02, -5.2549e-02,  1.8293e-02, -4.6609e-02,\n",
      "         -9.7340e-02,  9.2755e-03,  9.1867e-02, -3.9546e-02, -5.0251e-02,\n",
      "          2.5232e-02, -5.0176e-02, -4.5388e-02,  7.4674e-04,  1.1431e-02,\n",
      "          7.0852e-02, -1.1171e-02,  1.4328e-01,  7.2943e-02,  4.3729e-02,\n",
      "          4.7126e-02, -2.1214e-02, -3.7605e-02, -1.0752e-02,  1.4330e-02,\n",
      "         -9.5570e-04, -7.0456e-02, -1.7287e-02, -6.2811e-02,  2.4195e-02,\n",
      "         -1.3933e-01,  2.6549e-03,  6.0007e-02, -1.0736e-01,  3.3857e-02,\n",
      "         -8.7800e-03,  5.2284e-02,  4.4929e-02,  4.3290e-02, -9.9814e-02,\n",
      "         -3.0825e-02,  1.8337e-02, -9.4106e-03, -3.1660e-02,  6.6286e-03,\n",
      "          8.0725e-02, -5.7424e-02,  3.3407e-04,  5.2773e-02,  4.2248e-02,\n",
      "         -1.3652e-02,  5.9522e-02,  1.0129e-01, -1.0252e-01, -3.1620e-02,\n",
      "         -3.0927e-02, -5.7649e-02, -3.2964e-02, -4.4436e-02,  1.1286e-02,\n",
      "          1.1290e-02, -9.7263e-03,  3.5122e-02,  1.9145e-02,  3.9285e-02,\n",
      "          2.4979e-02, -5.6798e-02,  7.2913e-02, -6.7388e-02, -7.7330e-02,\n",
      "          6.2795e-02,  6.4434e-02,  7.3127e-02,  3.7289e-02, -2.3990e-02,\n",
      "         -1.1057e-01, -8.7222e-02,  4.9239e-02,  6.7422e-02,  5.2799e-02,\n",
      "          7.7512e-02,  7.4028e-02, -1.2066e-01,  1.2572e-02,  1.5012e-02,\n",
      "          5.2393e-02,  1.4407e-02,  7.1811e-03, -2.4598e-02, -2.7794e-02,\n",
      "          8.4707e-02, -3.1796e-02,  1.6741e-02,  6.1139e-02,  3.0692e-02,\n",
      "          8.7058e-02,  5.8987e-02, -7.9976e-03,  4.7452e-02, -1.7074e-02,\n",
      "          4.8237e-02,  2.7335e-03, -2.1023e-02, -7.5197e-04, -1.3585e-02,\n",
      "         -1.1437e-01, -1.0344e-01,  1.5145e-02,  1.4822e-01,  4.3539e-03,\n",
      "         -4.8190e-02, -1.9033e-02, -4.6459e-02]])\n",
      "Embeddings de las palabras: tensor([[[-0.0867,  0.0905,  0.0133,  ..., -0.0482, -0.0190, -0.0465],\n",
      "         [ 0.0652, -0.0678, -0.1393,  ...,  0.0322,  0.0991,  0.1104],\n",
      "         [-0.0032,  0.2392, -0.0245,  ..., -0.3839,  0.0199,  0.1572],\n",
      "         ...,\n",
      "         [-0.0129,  0.2340, -0.0339,  ...,  0.0248,  0.1979,  0.2229],\n",
      "         [-0.0739,  0.0814, -0.0025,  ..., -0.0717, -0.0166, -0.0734],\n",
      "         [-0.0039,  0.0613, -0.0262,  ...,  0.1493,  0.0737,  0.1269]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Carga el tokenizador y el modelo\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "#model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model_roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Oraci√≥n de ejemplo\n",
    "sentence = \"Hola, esto es una prueba con BERT.\"\n",
    "\n",
    "# Tokeniza la oraci√≥n\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Obtiene los embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model_roberta(**inputs)\n",
    "\n",
    "# Los embeddings de la √∫ltima capa oculta ser√≠an\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Embedding del [CLS] token que representa la oraci√≥n entera\n",
    "sentence_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "# Embeddings de cada token/palabra en la oraci√≥n\n",
    "word_embeddings = last_hidden_states\n",
    "\n",
    "print(\"Embedding de la oraci√≥n:\", sentence_embedding)\n",
    "print(\"Embeddings de las palabras:\", word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import json\n",
    "# Supongamos que tus datos est√°n en un archivo JSON llamado 'converted_data.jsonl'\n",
    "with open('../scripts/converted_data.jsonl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Convertir los datos en un DataFrame de pandas\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_word</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Behold, there came up out of the river seven c...</td>\n",
       "      <td>river</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am a fellow bondservant with you and with yo...</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The man, the lord of the land, said to us, 'By...</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shimei had sixteen sons and six daughters; but...</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"He has put my brothers far from me.</td>\n",
       "      <td>brothers</td>\n",
       "      <td>0.263889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence target_word  complexity\n",
       "0  Behold, there came up out of the river seven c...       river    0.000000\n",
       "1  I am a fellow bondservant with you and with yo...    brothers    0.000000\n",
       "2  The man, the lord of the land, said to us, 'By...    brothers    0.050000\n",
       "3  Shimei had sixteen sons and six daughters; but...    brothers    0.150000\n",
       "4               \"He has put my brothers far from me.    brothers    0.263889"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename columns\n",
    "df.rename(columns={'sentence': 'sentence', 'token': 'target_word', 'complexity': 'complexity'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_roberta.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6129/6129 [05:12<00:00, 19.63it/s]\n",
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6129/6129 [02:58<00:00, 34.34it/s]\n",
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1533/1533 [01:22<00:00, 18.55it/s]\n",
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1533/1533 [00:44<00:00, 34.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suponiendo que df es tu DataFrame que incluye las columnas 'sentence', 'target_word' y 'complexity'\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_embeddings(text_list, tokenizer, model, device):\n",
    "    \"\"\"Obtiene embeddings [CLS] para una lista de textos, con barra de progreso.\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for text in tqdm(text_list, desc=\"Processing\", leave=True):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded_input)\n",
    "        embeddings.append(output.last_hidden_state[:, 0, :].squeeze().cpu())\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "\n",
    "def process_dataframe(df, tokenizer, model, device):\n",
    "    \"\"\"Procesa un dataframe para obtener embeddings y complejidades.\"\"\"\n",
    "    sentence_embeddings = get_embeddings(df['sentence'].tolist(), tokenizer, model, device)\n",
    "    word_embeddings = get_embeddings(df['target_word'].tolist(), tokenizer, model, device)\n",
    "    complexities = torch.tensor(df['complexity'].values, dtype=torch.float).unsqueeze(1)\n",
    "    return torch.cat((sentence_embeddings, word_embeddings, complexities), dim=1)\n",
    "\n",
    "# Procesar los conjuntos de entrenamiento y prueba\n",
    "train_data = process_dataframe(train_df, tokenizer, model_roberta, device)\n",
    "test_data = process_dataframe(test_df, tokenizer, model_roberta, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6129, 1537]), torch.Size([1533, 1537]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entradas de entrenamiento (todas las columnas excepto la √∫ltima)\n",
    "train_features = train_data[:, :-1]\n",
    "# Objetivos de entrenamiento (√∫ltima columna)\n",
    "train_targets = train_data[:, -1]\n",
    "\n",
    "# Entradas de prueba\n",
    "test_features = test_data[:, :-1]\n",
    "# Objetivos de prueba\n",
    "test_targets = test_data[:, -1]\n",
    "\n",
    "# Aseg√∫rate de que los tensores est√©n en la CPU para convertirlos a arrays de NumPy\n",
    "train_features_np = train_features.numpy()\n",
    "train_targets_np = train_targets.numpy()\n",
    "test_features_np = test_features.numpy()\n",
    "test_targets_np = test_targets.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 13:29:21.173390: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-23 13:29:21.173797: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-23 13:29:21.176327: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-23 13:29:21.202044: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-23 13:29:21.828882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spoturno/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,472</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            ‚îÇ       \u001b[38;5;34m393,472\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            ‚îÇ        \u001b[38;5;34m32,896\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              ‚îÇ           \u001b[38;5;34m129\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">426,497</span> (1.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m426,497\u001b[0m (1.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">426,497</span> (1.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m426,497\u001b[0m (1.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Definir el modelo\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.15),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=rmse,\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1961 - mean_absolute_error: 0.1634 - val_loss: 0.1310 - val_mean_absolute_error: 0.0989\n",
      "Epoch 2/5\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1233 - mean_absolute_error: 0.0964 - val_loss: 0.1141 - val_mean_absolute_error: 0.0867\n",
      "Epoch 3/5\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1101 - mean_absolute_error: 0.0860 - val_loss: 0.1087 - val_mean_absolute_error: 0.0856\n",
      "Epoch 4/5\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1115 - mean_absolute_error: 0.0868 - val_loss: 0.1081 - val_mean_absolute_error: 0.0831\n",
      "Epoch 5/5\n",
      "\u001b[1m154/154\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1079 - mean_absolute_error: 0.0845 - val_loss: 0.1065 - val_mean_absolute_error: 0.0842\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "history = model.fit(train_features_np, train_targets_np,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=5,\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 - 0s - 836us/step - loss: 0.1084 - mean_absolute_error: 0.0843\n",
      "Test RMSE: 0.10837385058403015, Test MAE: 0.0842542052268982\n"
     ]
    }
   ],
   "source": [
    "# Evaluaci√≥n del modelo\n",
    "test_loss, test_mae = model.evaluate(test_features_np, test_targets_np, verbose=2)\n",
    "print(f\"Test RMSE: {test_loss}, Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelos/roberta.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>english</th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_word</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en_31</td>\n",
       "      <td>english</td>\n",
       "      <td>After Ron nearly dies drinking poisoned mead t...</td>\n",
       "      <td>distraught</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en_32</td>\n",
       "      <td>english</td>\n",
       "      <td>After Ron nearly dies drinking poisoned mead t...</td>\n",
       "      <td>drinking</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en_33</td>\n",
       "      <td>english</td>\n",
       "      <td>After the war, Hitler remained in the army and...</td>\n",
       "      <td>oratory</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en_34</td>\n",
       "      <td>english</td>\n",
       "      <td>After the war, Hitler remained in the army and...</td>\n",
       "      <td>reporting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en_35</td>\n",
       "      <td>english</td>\n",
       "      <td>After the war, Hitler remained in the army and...</td>\n",
       "      <td>infiltrating</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      en  english                                           sentence  \\\n",
       "0  en_31  english  After Ron nearly dies drinking poisoned mead t...   \n",
       "1  en_32  english  After Ron nearly dies drinking poisoned mead t...   \n",
       "2  en_33  english  After the war, Hitler remained in the army and...   \n",
       "3  en_34  english  After the war, Hitler remained in the army and...   \n",
       "4  en_35  english  After the war, Hitler remained in the army and...   \n",
       "\n",
       "    target_word  complexity  \n",
       "0    distraught         NaN  \n",
       "1      drinking         NaN  \n",
       "2       oratory         NaN  \n",
       "3     reporting         NaN  \n",
       "4  infiltrating         NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file_path = '/home/spoturno/coding/MLSP_Data/Data/Test/English/multilex_test_en_lcp_unlabelled.tsv'\n",
    "output_file_path = '/home/spoturno/coding/MLSP/predictions/roberta_with_mlp.tsv'\n",
    "\n",
    "df = pd.read_csv(input_file_path, delimiter='\\t', header=None, names=['en', 'english', 'sentence', 'target_word', 'complexity'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:26<00:00, 21.45it/s]\n",
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:17<00:00, 33.38it/s]\n"
     ]
    }
   ],
   "source": [
    "trial_set = process_dataframe(df, tokenizer, model_roberta, device)\n",
    "trial_set = trial_set[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.29537797],\n",
       "       [0.29974836],\n",
       "       [0.36869442],\n",
       "       [0.2598371 ],\n",
       "       [0.32287806],\n",
       "       [0.29097232],\n",
       "       [0.5070837 ],\n",
       "       [0.32218733],\n",
       "       [0.4086039 ],\n",
       "       [0.32529747],\n",
       "       [0.29130182],\n",
       "       [0.3068468 ],\n",
       "       [0.3155885 ],\n",
       "       [0.32339245],\n",
       "       [0.255053  ],\n",
       "       [0.25225905],\n",
       "       [0.2421126 ],\n",
       "       [0.23818764],\n",
       "       [0.23593727],\n",
       "       [0.22156873],\n",
       "       [0.48081815],\n",
       "       [0.30930048],\n",
       "       [0.2359544 ],\n",
       "       [0.40036413],\n",
       "       [0.3137411 ],\n",
       "       [0.3955433 ],\n",
       "       [0.3530424 ],\n",
       "       [0.25066715],\n",
       "       [0.24589984],\n",
       "       [0.24194309],\n",
       "       [0.3108501 ],\n",
       "       [0.34966725],\n",
       "       [0.27921954],\n",
       "       [0.3507047 ],\n",
       "       [0.39868677],\n",
       "       [0.33516228],\n",
       "       [0.28673303],\n",
       "       [0.30933803],\n",
       "       [0.2657011 ],\n",
       "       [0.29323632],\n",
       "       [0.23321757],\n",
       "       [0.33427608],\n",
       "       [0.29462826],\n",
       "       [0.31695133],\n",
       "       [0.41715893],\n",
       "       [0.47356415],\n",
       "       [0.47624847],\n",
       "       [0.32560158],\n",
       "       [0.2648496 ],\n",
       "       [0.2499236 ],\n",
       "       [0.3727569 ],\n",
       "       [0.27845728],\n",
       "       [0.34201702],\n",
       "       [0.28609204],\n",
       "       [0.2870801 ],\n",
       "       [0.30878693],\n",
       "       [0.37290812],\n",
       "       [0.3593338 ],\n",
       "       [0.36630172],\n",
       "       [0.33480787],\n",
       "       [0.27663684],\n",
       "       [0.39313695],\n",
       "       [0.32167026],\n",
       "       [0.38055575],\n",
       "       [0.30206457],\n",
       "       [0.32743278],\n",
       "       [0.26906854],\n",
       "       [0.42837334],\n",
       "       [0.28311467],\n",
       "       [0.34732324],\n",
       "       [0.26055297],\n",
       "       [0.35592484],\n",
       "       [0.2539689 ],\n",
       "       [0.2572217 ],\n",
       "       [0.2495819 ],\n",
       "       [0.20379321],\n",
       "       [0.35578203],\n",
       "       [0.2915178 ],\n",
       "       [0.30448535],\n",
       "       [0.30798346],\n",
       "       [0.29384556],\n",
       "       [0.28751385],\n",
       "       [0.2644757 ],\n",
       "       [0.30927673],\n",
       "       [0.24455084],\n",
       "       [0.3046853 ],\n",
       "       [0.27984354],\n",
       "       [0.32157493],\n",
       "       [0.34967536],\n",
       "       [0.344864  ],\n",
       "       [0.28806007],\n",
       "       [0.3298574 ],\n",
       "       [0.28693813],\n",
       "       [0.30811858],\n",
       "       [0.30352336],\n",
       "       [0.2502032 ],\n",
       "       [0.2267361 ],\n",
       "       [0.22676492],\n",
       "       [0.33265364],\n",
       "       [0.3416628 ],\n",
       "       [0.25506318],\n",
       "       [0.3442707 ],\n",
       "       [0.31602865],\n",
       "       [0.25428355],\n",
       "       [0.27853253],\n",
       "       [0.26447842],\n",
       "       [0.26146424],\n",
       "       [0.21946856],\n",
       "       [0.33398876],\n",
       "       [0.3252784 ],\n",
       "       [0.2177239 ],\n",
       "       [0.3086402 ],\n",
       "       [0.27845466],\n",
       "       [0.23778872],\n",
       "       [0.3396666 ],\n",
       "       [0.33703697],\n",
       "       [0.40927732],\n",
       "       [0.3077265 ],\n",
       "       [0.28034675],\n",
       "       [0.3415238 ],\n",
       "       [0.3152225 ],\n",
       "       [0.2968449 ],\n",
       "       [0.26560783],\n",
       "       [0.32718098],\n",
       "       [0.38786042],\n",
       "       [0.33268327],\n",
       "       [0.21457416],\n",
       "       [0.24295613],\n",
       "       [0.32622173],\n",
       "       [0.32866675],\n",
       "       [0.2989055 ],\n",
       "       [0.3494016 ],\n",
       "       [0.24401912],\n",
       "       [0.38893408],\n",
       "       [0.29606843],\n",
       "       [0.34943834],\n",
       "       [0.36773026],\n",
       "       [0.25113112],\n",
       "       [0.25680414],\n",
       "       [0.32531214],\n",
       "       [0.2810822 ],\n",
       "       [0.29682952],\n",
       "       [0.25598115],\n",
       "       [0.2963948 ],\n",
       "       [0.24013788],\n",
       "       [0.4360982 ],\n",
       "       [0.2782942 ],\n",
       "       [0.3424729 ],\n",
       "       [0.3407782 ],\n",
       "       [0.28456676],\n",
       "       [0.33870733],\n",
       "       [0.29159713],\n",
       "       [0.21807131],\n",
       "       [0.2642771 ],\n",
       "       [0.32877824],\n",
       "       [0.30973023],\n",
       "       [0.35981637],\n",
       "       [0.39745432],\n",
       "       [0.34655404],\n",
       "       [0.30222982],\n",
       "       [0.23292576],\n",
       "       [0.25369427],\n",
       "       [0.30162424],\n",
       "       [0.2571659 ],\n",
       "       [0.3045233 ],\n",
       "       [0.2798751 ],\n",
       "       [0.28526208],\n",
       "       [0.3075305 ],\n",
       "       [0.26150674],\n",
       "       [0.23312438],\n",
       "       [0.4107444 ],\n",
       "       [0.41276076],\n",
       "       [0.3681667 ],\n",
       "       [0.25645733],\n",
       "       [0.25006813],\n",
       "       [0.28430092],\n",
       "       [0.31744617],\n",
       "       [0.31050402],\n",
       "       [0.3113534 ],\n",
       "       [0.28127998],\n",
       "       [0.28401053],\n",
       "       [0.3599835 ],\n",
       "       [0.26526886],\n",
       "       [0.37253776],\n",
       "       [0.33174843],\n",
       "       [0.3515134 ],\n",
       "       [0.27321318],\n",
       "       [0.3636505 ],\n",
       "       [0.26579168],\n",
       "       [0.25628975],\n",
       "       [0.28681415],\n",
       "       [0.40715688],\n",
       "       [0.4907763 ],\n",
       "       [0.37117225],\n",
       "       [0.35424373],\n",
       "       [0.35229272],\n",
       "       [0.3079995 ],\n",
       "       [0.475124  ],\n",
       "       [0.37503743],\n",
       "       [0.41006768],\n",
       "       [0.32252818],\n",
       "       [0.29317033],\n",
       "       [0.5089423 ],\n",
       "       [0.28508657],\n",
       "       [0.33818197],\n",
       "       [0.30991253],\n",
       "       [0.27888617],\n",
       "       [0.235566  ],\n",
       "       [0.27916038],\n",
       "       [0.5200271 ],\n",
       "       [0.26004195],\n",
       "       [0.34168625],\n",
       "       [0.35069776],\n",
       "       [0.3043374 ],\n",
       "       [0.31064487],\n",
       "       [0.24910313],\n",
       "       [0.38277543],\n",
       "       [0.21437985],\n",
       "       [0.34763134],\n",
       "       [0.25167403],\n",
       "       [0.33681875],\n",
       "       [0.48701042],\n",
       "       [0.308824  ],\n",
       "       [0.4840039 ],\n",
       "       [0.2778309 ],\n",
       "       [0.30918294],\n",
       "       [0.38058335],\n",
       "       [0.36068445],\n",
       "       [0.26690322],\n",
       "       [0.32463986],\n",
       "       [0.29244697],\n",
       "       [0.2491515 ],\n",
       "       [0.3554005 ],\n",
       "       [0.34545234],\n",
       "       [0.3329094 ],\n",
       "       [0.35429734],\n",
       "       [0.26294944],\n",
       "       [0.26938024],\n",
       "       [0.26152855],\n",
       "       [0.42120263],\n",
       "       [0.30189863],\n",
       "       [0.41806293],\n",
       "       [0.34977773],\n",
       "       [0.3427276 ],\n",
       "       [0.4026375 ],\n",
       "       [0.3554002 ],\n",
       "       [0.3079251 ],\n",
       "       [0.6361801 ],\n",
       "       [0.33090216],\n",
       "       [0.36471832],\n",
       "       [0.42462212],\n",
       "       [0.24734047],\n",
       "       [0.26702982],\n",
       "       [0.2807479 ],\n",
       "       [0.25871778],\n",
       "       [0.33048388],\n",
       "       [0.3017194 ],\n",
       "       [0.3423668 ],\n",
       "       [0.35730988],\n",
       "       [0.27822208],\n",
       "       [0.47769415],\n",
       "       [0.35286146],\n",
       "       [0.30748123],\n",
       "       [0.26782733],\n",
       "       [0.30174267],\n",
       "       [0.31440097],\n",
       "       [0.2794652 ],\n",
       "       [0.45418668],\n",
       "       [0.33668905],\n",
       "       [0.31713593],\n",
       "       [0.3661431 ],\n",
       "       [0.3399953 ],\n",
       "       [0.28014684],\n",
       "       [0.280932  ],\n",
       "       [0.27271867],\n",
       "       [0.28802457],\n",
       "       [0.20175783],\n",
       "       [0.24625432],\n",
       "       [0.2757026 ],\n",
       "       [0.30092657],\n",
       "       [0.36587688],\n",
       "       [0.21891049],\n",
       "       [0.2975986 ],\n",
       "       [0.3058978 ],\n",
       "       [0.23635113],\n",
       "       [0.3131371 ],\n",
       "       [0.38610405],\n",
       "       [0.42571402],\n",
       "       [0.3526482 ],\n",
       "       [0.45340633],\n",
       "       [0.2802036 ],\n",
       "       [0.2506591 ],\n",
       "       [0.2876671 ],\n",
       "       [0.25506282],\n",
       "       [0.3383187 ],\n",
       "       [0.3948982 ],\n",
       "       [0.38215524],\n",
       "       [0.23638798],\n",
       "       [0.4659827 ],\n",
       "       [0.27902842],\n",
       "       [0.2617703 ],\n",
       "       [0.27464867],\n",
       "       [0.24110402],\n",
       "       [0.21056563],\n",
       "       [0.25808847],\n",
       "       [0.27622998],\n",
       "       [0.27208656],\n",
       "       [0.29597998],\n",
       "       [0.27766877],\n",
       "       [0.25493684],\n",
       "       [0.29261684],\n",
       "       [0.24143031],\n",
       "       [0.22894195],\n",
       "       [0.20902488],\n",
       "       [0.23847003],\n",
       "       [0.25235114],\n",
       "       [0.2223272 ],\n",
       "       [0.28974515],\n",
       "       [0.32134122],\n",
       "       [0.29625234],\n",
       "       [0.3826865 ],\n",
       "       [0.273624  ],\n",
       "       [0.29762322],\n",
       "       [0.35143656],\n",
       "       [0.31504345],\n",
       "       [0.313459  ],\n",
       "       [0.29674634],\n",
       "       [0.33420992],\n",
       "       [0.27333057],\n",
       "       [0.39381117],\n",
       "       [0.35029215],\n",
       "       [0.3362633 ],\n",
       "       [0.30483   ],\n",
       "       [0.24316783],\n",
       "       [0.316113  ],\n",
       "       [0.19089685],\n",
       "       [0.2637701 ],\n",
       "       [0.298469  ],\n",
       "       [0.45112562],\n",
       "       [0.44888046],\n",
       "       [0.4300436 ],\n",
       "       [0.28742975],\n",
       "       [0.31490391],\n",
       "       [0.39435235],\n",
       "       [0.2502867 ],\n",
       "       [0.34957626],\n",
       "       [0.4013468 ],\n",
       "       [0.31091827],\n",
       "       [0.22444728],\n",
       "       [0.23939812],\n",
       "       [0.32113948],\n",
       "       [0.24413484],\n",
       "       [0.36956978],\n",
       "       [0.2465469 ],\n",
       "       [0.24325259],\n",
       "       [0.34486958],\n",
       "       [0.23372558],\n",
       "       [0.3530587 ],\n",
       "       [0.27483517],\n",
       "       [0.3331302 ],\n",
       "       [0.22948009],\n",
       "       [0.41591123],\n",
       "       [0.33666903],\n",
       "       [0.40590835],\n",
       "       [0.37972492],\n",
       "       [0.30472046],\n",
       "       [0.28852773],\n",
       "       [0.28765756],\n",
       "       [0.37046742],\n",
       "       [0.28695744],\n",
       "       [0.349277  ],\n",
       "       [0.3957436 ],\n",
       "       [0.31211886],\n",
       "       [0.33143055],\n",
       "       [0.3184151 ],\n",
       "       [0.31720096],\n",
       "       [0.35768068],\n",
       "       [0.29781377],\n",
       "       [0.25721902],\n",
       "       [0.3034753 ],\n",
       "       [0.28739882],\n",
       "       [0.28916866],\n",
       "       [0.36725968],\n",
       "       [0.25762427],\n",
       "       [0.21862942],\n",
       "       [0.21274136],\n",
       "       [0.24777246],\n",
       "       [0.27426204],\n",
       "       [0.31865934],\n",
       "       [0.30235377],\n",
       "       [0.40725037],\n",
       "       [0.3392393 ],\n",
       "       [0.28324798],\n",
       "       [0.2521577 ],\n",
       "       [0.24205586],\n",
       "       [0.3916568 ],\n",
       "       [0.43545672],\n",
       "       [0.45955276],\n",
       "       [0.29000738],\n",
       "       [0.32521257],\n",
       "       [0.26433522],\n",
       "       [0.33950275],\n",
       "       [0.2800544 ],\n",
       "       [0.31559604],\n",
       "       [0.46209762],\n",
       "       [0.28844678],\n",
       "       [0.4468717 ],\n",
       "       [0.23175788],\n",
       "       [0.23438151],\n",
       "       [0.33103973],\n",
       "       [0.2179442 ],\n",
       "       [0.36041507],\n",
       "       [0.4122331 ],\n",
       "       [0.2921963 ],\n",
       "       [0.29365194],\n",
       "       [0.35031837],\n",
       "       [0.2813661 ],\n",
       "       [0.29176354],\n",
       "       [0.28842986],\n",
       "       [0.26181996],\n",
       "       [0.3504425 ],\n",
       "       [0.20677754],\n",
       "       [0.33238515],\n",
       "       [0.37681556],\n",
       "       [0.48236704],\n",
       "       [0.22386676],\n",
       "       [0.32462305],\n",
       "       [0.30764094],\n",
       "       [0.26320255],\n",
       "       [0.28395107],\n",
       "       [0.2992888 ],\n",
       "       [0.3514268 ],\n",
       "       [0.30284673],\n",
       "       [0.29458043],\n",
       "       [0.29013678],\n",
       "       [0.3644238 ],\n",
       "       [0.295861  ],\n",
       "       [0.21285929],\n",
       "       [0.22761567],\n",
       "       [0.33502248],\n",
       "       [0.2964164 ],\n",
       "       [0.279659  ],\n",
       "       [0.27982748],\n",
       "       [0.31411678],\n",
       "       [0.36339775],\n",
       "       [0.29591742],\n",
       "       [0.2640749 ],\n",
       "       [0.3416924 ],\n",
       "       [0.31018552],\n",
       "       [0.45369056],\n",
       "       [0.43134773],\n",
       "       [0.33818445],\n",
       "       [0.3053658 ],\n",
       "       [0.3283039 ],\n",
       "       [0.273271  ],\n",
       "       [0.22435528],\n",
       "       [0.24235734],\n",
       "       [0.24667141],\n",
       "       [0.37131196],\n",
       "       [0.28880405],\n",
       "       [0.24601193],\n",
       "       [0.31511998],\n",
       "       [0.23003453],\n",
       "       [0.24022746],\n",
       "       [0.2716279 ],\n",
       "       [0.3549346 ],\n",
       "       [0.34492105],\n",
       "       [0.309219  ],\n",
       "       [0.274812  ],\n",
       "       [0.26384008],\n",
       "       [0.2596859 ],\n",
       "       [0.36792797],\n",
       "       [0.27017474],\n",
       "       [0.23787335],\n",
       "       [0.22441643],\n",
       "       [0.3599164 ],\n",
       "       [0.24926955],\n",
       "       [0.3251731 ],\n",
       "       [0.27854478],\n",
       "       [0.2255618 ],\n",
       "       [0.32168984],\n",
       "       [0.29940414],\n",
       "       [0.29760233],\n",
       "       [0.27905813],\n",
       "       [0.2973814 ],\n",
       "       [0.27248266],\n",
       "       [0.2933878 ],\n",
       "       [0.3496245 ],\n",
       "       [0.24161315],\n",
       "       [0.3045336 ],\n",
       "       [0.27109042],\n",
       "       [0.34424394],\n",
       "       [0.28828093],\n",
       "       [0.3701499 ],\n",
       "       [0.3886717 ],\n",
       "       [0.3658877 ],\n",
       "       [0.351157  ],\n",
       "       [0.20141688],\n",
       "       [0.242796  ],\n",
       "       [0.25218767],\n",
       "       [0.2946798 ],\n",
       "       [0.42744797],\n",
       "       [0.28802702],\n",
       "       [0.30465883],\n",
       "       [0.40602112],\n",
       "       [0.27283427],\n",
       "       [0.24986538],\n",
       "       [0.27909726],\n",
       "       [0.36423594],\n",
       "       [0.34491074],\n",
       "       [0.37448126],\n",
       "       [0.33275592],\n",
       "       [0.3325327 ],\n",
       "       [0.33021194],\n",
       "       [0.23106384],\n",
       "       [0.44220483],\n",
       "       [0.2696221 ],\n",
       "       [0.39170188],\n",
       "       [0.23402897],\n",
       "       [0.41556424],\n",
       "       [0.35796583],\n",
       "       [0.23807941],\n",
       "       [0.26766858],\n",
       "       [0.32149833],\n",
       "       [0.38535562],\n",
       "       [0.33903942],\n",
       "       [0.3142905 ],\n",
       "       [0.31214428],\n",
       "       [0.21928133],\n",
       "       [0.34334046],\n",
       "       [0.40456682],\n",
       "       [0.37234166],\n",
       "       [0.23298463],\n",
       "       [0.22625197],\n",
       "       [0.24118106],\n",
       "       [0.34019738],\n",
       "       [0.23670775],\n",
       "       [0.21821734],\n",
       "       [0.21607395],\n",
       "       [0.2737471 ],\n",
       "       [0.43124515],\n",
       "       [0.30863667],\n",
       "       [0.23831978],\n",
       "       [0.22521234],\n",
       "       [0.45395344],\n",
       "       [0.21345867],\n",
       "       [0.24170303],\n",
       "       [0.29157454],\n",
       "       [0.36625654],\n",
       "       [0.33053085],\n",
       "       [0.27144307],\n",
       "       [0.38846508],\n",
       "       [0.4140989 ],\n",
       "       [0.37067282],\n",
       "       [0.27506763],\n",
       "       [0.3028456 ],\n",
       "       [0.38563836],\n",
       "       [0.2922606 ],\n",
       "       [0.27244595],\n",
       "       [0.27773935],\n",
       "       [0.27911785],\n",
       "       [0.2089133 ],\n",
       "       [0.25309855],\n",
       "       [0.2843521 ],\n",
       "       [0.21726277],\n",
       "       [0.2593819 ],\n",
       "       [0.28703064],\n",
       "       [0.3665168 ],\n",
       "       [0.3520373 ],\n",
       "       [0.39193645]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(trial_set.numpy())\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['complexity'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>english</th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_word</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en_31</td>\n",
       "      <td>english</td>\n",
       "      <td>After Ron nearly dies drinking poisoned mead t...</td>\n",
       "      <td>distraught</td>\n",
       "      <td>0.295378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en_32</td>\n",
       "      <td>english</td>\n",
       "      <td>After Ron nearly dies drinking poisoned mead t...</td>\n",
       "      <td>drinking</td>\n",
       "      <td>0.299748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en_33</td>\n",
       "      <td>english</td>\n",
       "      <td>After the war, Hitler remained in the army and...</td>\n",
       "      <td>oratory</td>\n",
       "      <td>0.368694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en_34</td>\n",
       "      <td>english</td>\n",
       "      <td>After the war, Hitler remained in the army and...</td>\n",
       "      <td>reporting</td>\n",
       "      <td>0.259837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en_35</td>\n",
       "      <td>english</td>\n",
       "      <td>After the war, Hitler remained in the army and...</td>\n",
       "      <td>infiltrating</td>\n",
       "      <td>0.322878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      en  english                                           sentence  \\\n",
       "0  en_31  english  After Ron nearly dies drinking poisoned mead t...   \n",
       "1  en_32  english  After Ron nearly dies drinking poisoned mead t...   \n",
       "2  en_33  english  After the war, Hitler remained in the army and...   \n",
       "3  en_34  english  After the war, Hitler remained in the army and...   \n",
       "4  en_35  english  After the war, Hitler remained in the army and...   \n",
       "\n",
       "    target_word  complexity  \n",
       "0    distraught    0.295378  \n",
       "1      drinking    0.299748  \n",
       "2       oratory    0.368694  \n",
       "3     reporting    0.259837  \n",
       "4  infiltrating    0.322878  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_file_path, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
