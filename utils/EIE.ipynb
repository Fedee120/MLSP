{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA = \"llama2-70b-4096\"\n",
    "GEMMA = \"gemma-7b-it\"\n",
    "MIXTRAL = \"mixtral-8x7b-32768\"\n",
    "\n",
    "oracion = \"El susurro del viento me transportó a un estado de serenidad profunda que no había experimentado antes.\"\n",
    "palabra = \"profunda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "\n",
    "def get_groq_simplifications(model: str, oracion: str, palabra: str):\n",
    "    client = Groq(\n",
    "        api_key=\"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\",\n",
    "    )\n",
    "\n",
    "    oracion_ejemplo = \"El resplandeciente sol iluminaba el paisaje selvático, creando un efecto casi mágico en la vegetación exuberante que se extendía a lo lejos.\"\n",
    "    palabra_ejemplo = \"mágico\"\n",
    "\n",
    "    params = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"\n",
    "                    Tarea: Sustituir una palabra en una oración sin cambiar el orden de las demás palabras.\n",
    "                    Respuesta: La respuesta solo incluye los candidatos. Tu respuesta no debe incluir todas las oraciones, solo las palabras por las que se quiere sustituir. La respuesta debe ser en el siguiente formato JSON:\n",
    "                        {{\n",
    "                            \"candidatos\": [\"candidato1\", \"candidato2\", \"candidato3\", \"candidato4\", \"candidato5\"],\n",
    "                        }}\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                    La oración original es: \"{oracion}\" La palabra a sustituir es \"{palabra}\".\n",
    "                    Instrucciones: Proporciona cinco candidatos para sustituir la palabra \"{palabra}\" en la oración, manteniendo el significado general y la coherencia de la oración. Los candidatos no deben alterar el orden de las demás palabras en la oración.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ],\n",
    "        \"model\": f\"{model}\",  # Aquí debes reemplazar \"model\" por el identificador de modelo real que deseas utilizar.\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "\n",
    "\n",
    "    if model != LLAMA: params[\"response_format\"] = { \"type\": \"json_object\" }\n",
    "\n",
    "    chat_completion = client.chat.completions.create(**params)\n",
    "\n",
    "    output = json.loads(chat_completion.choices[0].message.content)\n",
    "    output['candidatos'].append(palabra)\n",
    "\n",
    "    return output\n",
    "\n",
    "#get_groq_simplifications(GEMMA, oracion, palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/CREA_total.TXT', sep='\\t', header=None,\n",
    "                 names=['Orden', 'Palabra', 'Frec.absoluta', 'Frec.normalizada'],\n",
    "                 encoding='ISO 8859-1', low_memory=False,\n",
    "                 dtype={'Orden': str, 'Palabra': str, 'Frec.absoluta': str, 'Frec.normalizada': float})\n",
    "\n",
    "\n",
    "# Asegurarse de que 'Frec.normalizada' se interprete como numérico\n",
    "df['Frec.normalizada'] = pd.to_numeric(df['Frec.normalizada'], errors='coerce')\n",
    "\n",
    "# Ordenar el DataFrame por 'Frec.normalizada' para un mejor gráfico\n",
    "df_sorted = df.sort_values(by='Frec.normalizada', ascending=True)\n",
    "\n",
    "# Suponiendo que ya has cargado los datos en el DataFrame df\n",
    "# Convertir explícitamente la columna 'Palabra' a cadenas de texto\n",
    "df['Palabra'] = df['Palabra'].astype(str)\n",
    "\n",
    "# Ordenar el DataFrame por 'Frec.normalizada' para un mejor gráfico\n",
    "df_sorted = df.sort_values(by='Frec.normalizada', ascending=True)\n",
    "\n",
    "# Convertir 'Frec.normalizada' a numérico, por si acaso no lo está\n",
    "df['Frec.normalizada'] = pd.to_numeric(df['Frec.normalizada'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orden</th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Frec.absoluta</th>\n",
       "      <th>Frec.normalizada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Orden</td>\n",
       "      <td>Frec.absoluta</td>\n",
       "      <td>Frec.normalizada</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.</td>\n",
       "      <td>de</td>\n",
       "      <td>9,999,518</td>\n",
       "      <td>65545.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.</td>\n",
       "      <td>la</td>\n",
       "      <td>6,277,560</td>\n",
       "      <td>41148.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.</td>\n",
       "      <td>que</td>\n",
       "      <td>4,681,839</td>\n",
       "      <td>30688.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.</td>\n",
       "      <td>el</td>\n",
       "      <td>4,569,652</td>\n",
       "      <td>29953.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Orden         Palabra       Frec.absoluta  Frec.normalizada\n",
       "0       Orden  Frec.absoluta    Frec.normalizada                NaN\n",
       "1          1.              de          9,999,518           65545.55\n",
       "2          2.              la          6,277,560           41148.59\n",
       "3          3.            que           4,681,839           30688.85\n",
       "4          4.              el          4,569,652           29953.48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import json\n",
    "\n",
    "# Función para remover acentos y caracteres diacríticos\n",
    "def remover_acentos(input_str):\n",
    "    if isinstance(input_str, float):  # Verifica si el valor es un float (lo que usualmente indica un NaN en pandas)\n",
    "        return \"\"  # Devuelve una cadena vacía si el valor es NaN\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join(c for c in nfkd_form if not unicodedata.combining(c))\n",
    "\n",
    "def obtener_frecuencias_de_sustitutos(candidatos):\n",
    "    # Leer el DataFrame asegurándose de que las columnas tienen el tipo correcto\n",
    "    # Es posible que necesites ajustar los tipos dependiendo de los datos\n",
    "    df = pd.read_csv('data/CREA_total.TXT', sep='\\t', header=None,\n",
    "                    names=['Orden', 'Palabra', 'Frec.absoluta', 'Frec.normalizada'],\n",
    "                    encoding='ISO 8859-1', low_memory=False,\n",
    "                    dtype={'Orden': str, 'Palabra': str, 'Frec.absoluta': str, 'Frec.normalizada': float})\n",
    "\n",
    "    # Crear una nueva lista con las palabras sin acentos y en minúsculas\n",
    "    candidatos_minuscula = [remover_acentos(palabra).lower() for palabra in candidatos[\"candidatos\"]]\n",
    "\n",
    "    # Normalizar la columna 'Palabra' en el DataFrame para que coincida con la lista de candidatos\n",
    "    df['Palabra'] = df['Palabra'].astype(str).apply(remover_acentos).str.lower().str.strip()\n",
    "\n",
    "    # Filtrar el DataFrame para obtener solo las filas que contienen las palabras candidatas\n",
    "    df_filtrado = df[df['Palabra'].isin(candidatos_minuscula)]\n",
    "\n",
    "    df_unicas_filtradas = df_filtrado.drop_duplicates(subset='Palabra')\n",
    "\n",
    "    return df_unicas_filtradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_predicciones(oracion_p: str, palabra_p: str):\n",
    "    candidatos_mixtral = get_groq_simplifications(MIXTRAL, oracion_p, palabra_p)\n",
    "    df_filtrado_mixtral = obtener_frecuencias_de_sustitutos(candidatos_mixtral)\n",
    "\n",
    "    candidatos_llama = get_groq_simplifications(LLAMA, oracion_p, palabra_p)\n",
    "    df_filtrado_llama = obtener_frecuencias_de_sustitutos(candidatos_llama)\n",
    "\n",
    "    candidatos_gemma = get_groq_simplifications(GEMMA, oracion_p, palabra_p)\n",
    "    df_filtrado_gemma = obtener_frecuencias_de_sustitutos(candidatos_gemma)\n",
    "\n",
    "    # combinamos filtrados de los modelos\n",
    "    df_merged = pd.concat([df_filtrado_mixtral, df_filtrado_llama, df_filtrado_gemma], ignore_index=True)\n",
    "\n",
    "    # Ordenamos segun frecuencia absoluta\n",
    "    df_merged = df_merged.sort_values('Frec.absoluta', ascending=False)\n",
    "\n",
    "    # Elimino duplicados\n",
    "    df_merged = df_merged.drop_duplicates(subset='Palabra')\n",
    "\n",
    "    return df_merged[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff\n",
    "\n",
    "# # Define your backoff strategy here\n",
    "# @backoff.on_exception(backoff.expo,\n",
    "#                       Exception,  # Or a more specific exception based on what obtener_predicciones might raise\n",
    "#                       max_time=300)  # Total max time to retry in seconds\n",
    "def call_api_with_backoff(sentence, target_word):\n",
    "    return obtener_predicciones_v2(sentence, target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linea 1/30 ✓\n",
      "Linea 2/30 ✓\n",
      "Linea 3/30 ✓\n",
      "Linea 4/30 ✓\n",
      "Linea 5/30 ✓\n",
      "Linea 6/30 ✓\n",
      "Linea 7/30 ✓\n",
      "Linea 8/30 ✓\n",
      "Linea 9/30 ✓\n",
      "Linea 10/30 ✓\n",
      "Linea 11/30 ✓\n",
      "Linea 12/30 ✓\n",
      "Linea 13/30 ✓\n",
      "Linea 14/30 ✓\n",
      "Linea 15/30 ✓\n",
      "Linea 16/30 ✓\n",
      "Linea 17/30 ✓\n",
      "Error procesando linea 18: Extra data: line 11 column 1 (char 292)\n",
      "Linea 19/30 ✓\n",
      "Linea 20/30 ✓\n",
      "Linea 21/30 ✓\n",
      "Linea 22/30 ✓\n",
      "Linea 23/30 ✓\n",
      "Linea 24/30 ✓\n",
      "Linea 25/30 ✓\n",
      "Linea 26/30 ✓\n",
      "Linea 27/30 ✓\n",
      "Linea 28/30 ✓\n",
      "Linea 29/30 ✓\n",
      "Linea 30/30 ✓\n",
      "Linea 31/30 ✓\n",
      "Linea 32/30 ✓\n",
      "Linea 33/30 ✓\n",
      "Linea 34/30 ✓\n",
      "Linea 35/30 ✓\n",
      "Linea 36/30 ✓\n",
      "Linea 37/30 ✓\n",
      "Linea 38/30 ✓\n",
      "Linea 39/30 ✓\n",
      "Linea 40/30 ✓\n",
      "Linea 41/30 ✓\n",
      "Linea 42/30 ✓\n",
      "Linea 43/30 ✓\n",
      "Linea 44/30 ✓\n",
      "Linea 45/30 ✓\n",
      "Error procesando linea 46: Extra data: line 11 column 1 (char 281)\n",
      "Linea 47/30 ✓\n",
      "Linea 48/30 ✓\n",
      "Linea 49/30 ✓\n",
      "Linea 50/30 ✓\n",
      "Error procesando linea 51: Extra data: line 11 column 1 (char 299)\n",
      "Linea 52/30 ✓\n",
      "Linea 53/30 ✓\n",
      "Linea 54/30 ✓\n",
      "Error procesando linea 55: Extra data: line 11 column 1 (char 82)\n",
      "Linea 56/30 ✓\n",
      "Linea 57/30 ✓\n",
      "Linea 58/30 ✓\n",
      "Linea 59/30 ✓\n",
      "Linea 60/30 ✓\n",
      "Linea 61/30 ✓\n",
      "Linea 62/30 ✓\n",
      "Linea 63/30 ✓\n",
      "Linea 64/30 ✓\n",
      "Linea 65/30 ✓\n",
      "Linea 66/30 ✓\n",
      "Linea 67/30 ✓\n",
      "Linea 68/30 ✓\n",
      "Linea 69/30 ✓\n",
      "Linea 70/30 ✓\n",
      "Linea 71/30 ✓\n",
      "Linea 72/30 ✓\n",
      "Linea 73/30 ✓\n",
      "Linea 74/30 ✓\n",
      "Linea 75/30 ✓\n",
      "Linea 76/30 ✓\n",
      "Linea 77/30 ✓\n",
      "Linea 78/30 ✓\n",
      "Linea 79/30 ✓\n",
      "Linea 80/30 ✓\n",
      "Linea 81/30 ✓\n",
      "Linea 82/30 ✓\n",
      "Linea 83/30 ✓\n",
      "Linea 84/30 ✓\n",
      "Linea 85/30 ✓\n",
      "Linea 86/30 ✓\n",
      "Linea 87/30 ✓\n",
      "Linea 88/30 ✓\n",
      "Linea 89/30 ✓\n",
      "Linea 90/30 ✓\n",
      "Linea 91/30 ✓\n",
      "Linea 92/30 ✓\n",
      "Linea 93/30 ✓\n",
      "Linea 94/30 ✓\n",
      "Linea 95/30 ✓\n",
      "Linea 96/30 ✓\n",
      "Linea 97/30 ✓\n",
      "Linea 98/30 ✓\n",
      "Linea 99/30 ✓\n",
      "Linea 100/30 ✓\n",
      "Linea 101/30 ✓\n",
      "Error procesando linea 102: Error code: 400 - {'error': {'message': 'Failed to generate JSON. Please adjust your prompt. Failed output:\\nailed: expected `{`', 'type': 'invalid_request_error', 'code': 'json_validate_failed'}}\n",
      "Linea 103/30 ✓\n",
      "Linea 104/30 ✓\n",
      "Linea 105/30 ✓\n",
      "Linea 106/30 ✓\n",
      "Linea 107/30 ✓\n",
      "Linea 108/30 ✓\n",
      "Linea 109/30 ✓\n",
      "Linea 110/30 ✓\n",
      "Linea 111/30 ✓\n",
      "Linea 112/30 ✓\n",
      "Linea 113/30 ✓\n",
      "Linea 114/30 ✓\n",
      "Linea 115/30 ✓\n",
      "Linea 116/30 ✓\n",
      "Linea 117/30 ✓\n",
      "Linea 118/30 ✓\n",
      "Linea 119/30 ✓\n",
      "Linea 120/30 ✓\n",
      "Linea 121/30 ✓\n",
      "Linea 122/30 ✓\n",
      "Linea 123/30 ✓\n",
      "Linea 124/30 ✓\n",
      "Linea 125/30 ✓\n",
      "Linea 126/30 ✓\n",
      "Linea 127/30 ✓\n",
      "Linea 128/30 ✓\n",
      "Linea 129/30 ✓\n",
      "Linea 130/30 ✓\n",
      "Linea 131/30 ✓\n",
      "Linea 132/30 ✓\n",
      "Linea 133/30 ✓\n",
      "Linea 134/30 ✓\n",
      "Error procesando linea 135: Extra data: line 11 column 1 (char 303)\n",
      "Linea 136/30 ✓\n",
      "Linea 137/30 ✓\n",
      "Linea 138/30 ✓\n",
      "Linea 139/30 ✓\n",
      "Linea 140/30 ✓\n",
      "Linea 141/30 ✓\n",
      "Linea 142/30 ✓\n",
      "Linea 143/30 ✓\n",
      "Linea 144/30 ✓\n",
      "Error procesando linea 145: Extra data: line 11 column 1 (char 304)\n",
      "Linea 146/30 ✓\n",
      "Linea 147/30 ✓\n",
      "Linea 148/30 ✓\n",
      "Linea 149/30 ✓\n",
      "Linea 150/30 ✓\n",
      "Linea 151/30 ✓\n",
      "Linea 152/30 ✓\n",
      "Linea 153/30 ✓\n",
      "Linea 154/30 ✓\n",
      "Linea 155/30 ✓\n",
      "Linea 156/30 ✓\n",
      "Linea 157/30 ✓\n",
      "Linea 158/30 ✓\n",
      "Linea 159/30 ✓\n",
      "Linea 160/30 ✓\n",
      "Linea 161/30 ✓\n",
      "Linea 162/30 ✓\n",
      "Linea 163/30 ✓\n",
      "Linea 164/30 ✓\n",
      "Linea 165/30 ✓\n",
      "Linea 166/30 ✓\n",
      "Linea 167/30 ✓\n",
      "Linea 168/30 ✓\n",
      "Linea 169/30 ✓\n",
      "Linea 170/30 ✓\n",
      "Linea 171/30 ✓\n",
      "Linea 172/30 ✓\n",
      "Linea 173/30 ✓\n",
      "Linea 174/30 ✓\n",
      "Linea 175/30 ✓\n",
      "Linea 176/30 ✓\n",
      "Linea 177/30 ✓\n",
      "Linea 178/30 ✓\n",
      "Linea 179/30 ✓\n",
      "Linea 180/30 ✓\n",
      "Linea 181/30 ✓\n",
      "Error procesando linea 182: Extra data: line 11 column 1 (char 80)\n",
      "Error procesando linea 183: Extra data: line 11 column 1 (char 305)\n",
      "Linea 184/30 ✓\n",
      "Linea 185/30 ✓\n",
      "Linea 186/30 ✓\n",
      "Linea 187/30 ✓\n",
      "Linea 188/30 ✓\n",
      "Linea 189/30 ✓\n",
      "Linea 190/30 ✓\n",
      "Linea 191/30 ✓\n",
      "Linea 192/30 ✓\n",
      "Linea 193/30 ✓\n",
      "Linea 194/30 ✓\n",
      "Linea 195/30 ✓\n",
      "Linea 196/30 ✓\n",
      "Linea 197/30 ✓\n",
      "Linea 198/30 ✓\n",
      "Linea 199/30 ✓\n",
      "Linea 200/30 ✓\n",
      "Linea 201/30 ✓\n",
      "Linea 202/30 ✓\n",
      "Linea 203/30 ✓\n",
      "Linea 204/30 ✓\n",
      "Linea 205/30 ✓\n",
      "Linea 206/30 ✓\n",
      "Linea 207/30 ✓\n",
      "Linea 208/30 ✓\n",
      "Linea 209/30 ✓\n",
      "Linea 210/30 ✓\n",
      "Linea 211/30 ✓\n",
      "Linea 212/30 ✓\n",
      "Linea 213/30 ✓\n",
      "Linea 214/30 ✓\n",
      "Linea 215/30 ✓\n",
      "Linea 216/30 ✓\n",
      "Linea 217/30 ✓\n",
      "Linea 218/30 ✓\n",
      "Linea 219/30 ✓\n",
      "Linea 220/30 ✓\n",
      "Linea 221/30 ✓\n",
      "Linea 222/30 ✓\n",
      "Linea 223/30 ✓\n",
      "Linea 224/30 ✓\n",
      "Linea 225/30 ✓\n",
      "Linea 226/30 ✓\n",
      "Linea 227/30 ✓\n",
      "Linea 228/30 ✓\n",
      "Linea 229/30 ✓\n",
      "Error procesando linea 230: 'candidatos'\n",
      "Error procesando linea 231: Extra data: line 11 column 1 (char 295)\n",
      "Linea 232/30 ✓\n",
      "Linea 233/30 ✓\n",
      "Linea 234/30 ✓\n",
      "Linea 235/30 ✓\n",
      "Linea 236/30 ✓\n",
      "Linea 237/30 ✓\n",
      "Linea 238/30 ✓\n",
      "Linea 239/30 ✓\n",
      "Linea 240/30 ✓\n",
      "Linea 241/30 ✓\n",
      "Linea 242/30 ✓\n",
      "Linea 243/30 ✓\n",
      "Linea 244/30 ✓\n",
      "Error procesando linea 245: Extra data: line 11 column 1 (char 293)\n",
      "Linea 246/30 ✓\n",
      "Linea 247/30 ✓\n",
      "Linea 248/30 ✓\n",
      "Linea 249/30 ✓\n",
      "Linea 250/30 ✓\n",
      "Linea 251/30 ✓\n",
      "Linea 252/30 ✓\n",
      "Linea 253/30 ✓\n",
      "Linea 254/30 ✓\n",
      "Linea 255/30 ✓\n",
      "Linea 256/30 ✓\n",
      "Linea 257/30 ✓\n",
      "Linea 258/30 ✓\n",
      "Linea 259/30 ✓\n",
      "Linea 260/30 ✓\n",
      "Linea 261/30 ✓\n",
      "Linea 262/30 ✓\n",
      "Error procesando linea 263: Extra data: line 11 column 1 (char 272)\n",
      "Linea 264/30 ✓\n",
      "Linea 265/30 ✓\n",
      "Linea 266/30 ✓\n",
      "Linea 267/30 ✓\n",
      "Linea 268/30 ✓\n",
      "Linea 269/30 ✓\n",
      "Linea 270/30 ✓\n",
      "Linea 271/30 ✓\n",
      "Linea 272/30 ✓\n",
      "Linea 273/30 ✓\n",
      "Linea 274/30 ✓\n",
      "Linea 275/30 ✓\n",
      "Linea 276/30 ✓\n",
      "Linea 277/30 ✓\n",
      "Linea 278/30 ✓\n",
      "Error procesando linea 279: Extra data: line 11 column 1 (char 258)\n",
      "Linea 280/30 ✓\n",
      "Linea 281/30 ✓\n",
      "Linea 282/30 ✓\n",
      "Linea 283/30 ✓\n",
      "Linea 284/30 ✓\n",
      "Linea 285/30 ✓\n",
      "Linea 286/30 ✓\n",
      "Linea 287/30 ✓\n",
      "Linea 288/30 ✓\n",
      "Linea 289/30 ✓\n",
      "Linea 290/30 ✓\n",
      "Linea 291/30 ✓\n",
      "Linea 292/30 ✓\n",
      "Linea 293/30 ✓\n",
      "Linea 294/30 ✓\n",
      "Linea 295/30 ✓\n",
      "Linea 296/30 ✓\n",
      "Linea 297/30 ✓\n",
      "Linea 298/30 ✓\n",
      "Linea 299/30 ✓\n",
      "Linea 300/30 ✓\n",
      "Linea 301/30 ✓\n",
      "Linea 302/30 ✓\n",
      "Linea 303/30 ✓\n",
      "Error procesando linea 304: Extra data: line 11 column 1 (char 122)\n",
      "Linea 305/30 ✓\n",
      "Linea 306/30 ✓\n",
      "Linea 307/30 ✓\n",
      "Linea 308/30 ✓\n",
      "Linea 309/30 ✓\n",
      "Linea 310/30 ✓\n",
      "Linea 311/30 ✓\n",
      "Error procesando linea 312: Extra data: line 11 column 1 (char 306)\n",
      "Linea 313/30 ✓\n",
      "Linea 314/30 ✓\n",
      "Error procesando linea 315: Error code: 400 - {'error': {'message': 'Failed to generate JSON. Please adjust your prompt. Failed output:\\nailed: expected `{`', 'type': 'invalid_request_error', 'code': 'json_validate_failed'}}\n",
      "Linea 316/30 ✓\n",
      "Linea 317/30 ✓\n",
      "Linea 318/30 ✓\n",
      "Linea 319/30 ✓\n",
      "Linea 320/30 ✓\n",
      "Linea 321/30 ✓\n",
      "Linea 322/30 ✓\n",
      "Linea 323/30 ✓\n",
      "Linea 324/30 ✓\n",
      "Linea 325/30 ✓\n",
      "Linea 326/30 ✓\n",
      "Linea 327/30 ✓\n",
      "Linea 328/30 ✓\n",
      "Linea 329/30 ✓\n",
      "Error procesando linea 330: Extra data: line 11 column 1 (char 287)\n",
      "Linea 331/30 ✓\n",
      "Linea 332/30 ✓\n",
      "Linea 333/30 ✓\n",
      "Linea 334/30 ✓\n",
      "Linea 335/30 ✓\n",
      "Linea 336/30 ✓\n",
      "Linea 337/30 ✓\n",
      "Linea 338/30 ✓\n",
      "Linea 339/30 ✓\n",
      "Linea 340/30 ✓\n",
      "Linea 341/30 ✓\n",
      "Error procesando linea 342: Extra data: line 11 column 1 (char 298)\n",
      "Linea 343/30 ✓\n",
      "Linea 344/30 ✓\n",
      "Error procesando linea 345: Extra data: line 11 column 1 (char 299)\n",
      "Linea 346/30 ✓\n",
      "Linea 347/30 ✓\n",
      "Linea 348/30 ✓\n",
      "Linea 349/30 ✓\n",
      "Linea 350/30 ✓\n",
      "Linea 351/30 ✓\n",
      "Linea 352/30 ✓\n",
      "Linea 353/30 ✓\n",
      "Linea 354/30 ✓\n",
      "Linea 355/30 ✓\n",
      "Linea 356/30 ✓\n",
      "Linea 357/30 ✓\n",
      "Error procesando linea 358: Extra data: line 5 column 1 (char 127)\n",
      "Linea 359/30 ✓\n",
      "Linea 360/30 ✓\n",
      "Linea 361/30 ✓\n",
      "Linea 362/30 ✓\n",
      "Linea 363/30 ✓\n",
      "Linea 364/30 ✓\n",
      "Linea 365/30 ✓\n",
      "Linea 366/30 ✓\n",
      "Linea 367/30 ✓\n",
      "Linea 368/30 ✓\n",
      "Linea 369/30 ✓\n",
      "Linea 370/30 ✓\n",
      "Linea 371/30 ✓\n",
      "Linea 372/30 ✓\n",
      "Linea 373/30 ✓\n",
      "Linea 374/30 ✓\n",
      "Linea 375/30 ✓\n",
      "Linea 376/30 ✓\n",
      "Error procesando linea 377: Extra data: line 11 column 1 (char 292)\n",
      "Linea 378/30 ✓\n",
      "Linea 379/30 ✓\n",
      "Linea 380/30 ✓\n",
      "Linea 381/30 ✓\n",
      "Error procesando linea 382: Extra data: line 11 column 1 (char 87)\n",
      "Linea 383/30 ✓\n",
      "Linea 384/30 ✓\n",
      "Linea 385/30 ✓\n",
      "Linea 386/30 ✓\n",
      "Linea 387/30 ✓\n",
      "Linea 388/30 ✓\n",
      "Linea 389/30 ✓\n",
      "Linea 390/30 ✓\n",
      "Linea 391/30 ✓\n",
      "Linea 392/30 ✓\n",
      "Linea 393/30 ✓\n",
      "Linea 394/30 ✓\n",
      "Linea 395/30 ✓\n",
      "Linea 396/30 ✓\n",
      "Linea 397/30 ✓\n",
      "Linea 398/30 ✓\n",
      "Linea 399/30 ✓\n",
      "Linea 400/30 ✓\n",
      "Linea 401/30 ✓\n",
      "Linea 402/30 ✓\n",
      "Linea 403/30 ✓\n",
      "Linea 404/30 ✓\n",
      "Linea 405/30 ✓\n",
      "Linea 406/30 ✓\n",
      "Linea 407/30 ✓\n",
      "Linea 408/30 ✓\n",
      "Linea 409/30 ✓\n",
      "Linea 410/30 ✓\n",
      "Linea 411/30 ✓\n",
      "Linea 412/30 ✓\n",
      "Linea 413/30 ✓\n",
      "Linea 414/30 ✓\n",
      "Linea 415/30 ✓\n",
      "Linea 416/30 ✓\n",
      "Linea 417/30 ✓\n",
      "Linea 418/30 ✓\n",
      "Linea 419/30 ✓\n",
      "Linea 420/30 ✓\n",
      "Linea 421/30 ✓\n",
      "Linea 422/30 ✓\n",
      "Linea 423/30 ✓\n",
      "Linea 424/30 ✓\n",
      "Linea 425/30 ✓\n",
      "Linea 426/30 ✓\n",
      "Linea 427/30 ✓\n",
      "Linea 428/30 ✓\n",
      "Linea 429/30 ✓\n",
      "Error procesando linea 430: Extra data: line 11 column 1 (char 294)\n",
      "Linea 431/30 ✓\n",
      "Linea 432/30 ✓\n",
      "Linea 433/30 ✓\n",
      "Linea 434/30 ✓\n",
      "Linea 435/30 ✓\n",
      "Linea 436/30 ✓\n",
      "Linea 437/30 ✓\n",
      "Linea 438/30 ✓\n",
      "Error procesando linea 439: Extra data: line 11 column 1 (char 92)\n",
      "Error procesando linea 440: Extra data: line 11 column 1 (char 256)\n",
      "Linea 441/30 ✓\n",
      "Linea 442/30 ✓\n",
      "Linea 443/30 ✓\n",
      "Linea 444/30 ✓\n",
      "Linea 445/30 ✓\n",
      "Linea 446/30 ✓\n",
      "Linea 447/30 ✓\n",
      "Linea 448/30 ✓\n",
      "Linea 449/30 ✓\n",
      "Linea 450/30 ✓\n",
      "Linea 451/30 ✓\n",
      "Linea 452/30 ✓\n",
      "Linea 453/30 ✓\n",
      "Linea 454/30 ✓\n",
      "Linea 455/30 ✓\n",
      "Linea 456/30 ✓\n",
      "Linea 457/30 ✓\n",
      "Linea 458/30 ✓\n",
      "Linea 459/30 ✓\n",
      "Linea 460/30 ✓\n",
      "Linea 461/30 ✓\n",
      "Linea 462/30 ✓\n",
      "Linea 463/30 ✓\n",
      "Linea 464/30 ✓\n",
      "Linea 465/30 ✓\n",
      "Linea 466/30 ✓\n",
      "Linea 467/30 ✓\n",
      "Linea 468/30 ✓\n",
      "Linea 469/30 ✓\n",
      "Linea 470/30 ✓\n",
      "Linea 471/30 ✓\n",
      "Linea 472/30 ✓\n",
      "Linea 473/30 ✓\n",
      "Linea 474/30 ✓\n",
      "Linea 475/30 ✓\n",
      "Linea 476/30 ✓\n",
      "Linea 477/30 ✓\n",
      "Linea 478/30 ✓\n",
      "Linea 479/30 ✓\n",
      "Linea 480/30 ✓\n",
      "Linea 481/30 ✓\n",
      "Linea 482/30 ✓\n",
      "Linea 483/30 ✓\n",
      "Linea 484/30 ✓\n",
      "Linea 485/30 ✓\n",
      "Linea 486/30 ✓\n",
      "Linea 487/30 ✓\n",
      "Linea 488/30 ✓\n",
      "Linea 489/30 ✓\n",
      "Linea 490/30 ✓\n",
      "Linea 491/30 ✓\n",
      "Linea 492/30 ✓\n",
      "Linea 493/30 ✓\n",
      "Linea 494/30 ✓\n",
      "Linea 495/30 ✓\n",
      "Linea 496/30 ✓\n",
      "Linea 497/30 ✓\n",
      "Linea 498/30 ✓\n",
      "Linea 499/30 ✓\n",
      "Linea 500/30 ✓\n",
      "Linea 501/30 ✓\n",
      "Linea 502/30 ✓\n",
      "Linea 503/30 ✓\n",
      "Linea 504/30 ✓\n",
      "Linea 505/30 ✓\n",
      "Linea 506/30 ✓\n",
      "Linea 507/30 ✓\n",
      "Linea 508/30 ✓\n",
      "Error procesando linea 509: Extra data: line 11 column 1 (char 267)\n",
      "Linea 510/30 ✓\n",
      "Linea 511/30 ✓\n",
      "Linea 512/30 ✓\n",
      "Linea 513/30 ✓\n",
      "Linea 514/30 ✓\n",
      "Linea 515/30 ✓\n",
      "Linea 516/30 ✓\n",
      "Linea 517/30 ✓\n",
      "Linea 518/30 ✓\n",
      "Linea 519/30 ✓\n",
      "Linea 520/30 ✓\n",
      "Linea 521/30 ✓\n",
      "Linea 522/30 ✓\n",
      "Linea 523/30 ✓\n",
      "Linea 524/30 ✓\n",
      "Linea 525/30 ✓\n",
      "Linea 526/30 ✓\n",
      "Linea 527/30 ✓\n",
      "Linea 528/30 ✓\n",
      "Linea 529/30 ✓\n",
      "Linea 530/30 ✓\n",
      "Linea 531/30 ✓\n",
      "Linea 532/30 ✓\n",
      "Linea 533/30 ✓\n",
      "Linea 534/30 ✓\n",
      "Linea 535/30 ✓\n",
      "Linea 536/30 ✓\n",
      "Error procesando linea 537: Extra data: line 11 column 1 (char 85)\n",
      "Linea 538/30 ✓\n",
      "Linea 539/30 ✓\n",
      "Error procesando linea 540: Error code: 400 - {'error': {'message': 'Failed to generate JSON. Please adjust your prompt. Failed output:\\nailed: expected `{`', 'type': 'invalid_request_error', 'code': 'json_validate_failed'}}\n",
      "Linea 541/30 ✓\n",
      "Linea 542/30 ✓\n",
      "Linea 543/30 ✓\n",
      "Linea 544/30 ✓\n",
      "Linea 545/30 ✓\n",
      "Linea 546/30 ✓\n",
      "Linea 547/30 ✓\n",
      "Linea 548/30 ✓\n",
      "Linea 549/30 ✓\n",
      "Linea 550/30 ✓\n",
      "Linea 551/30 ✓\n",
      "Linea 552/30 ✓\n",
      "Linea 553/30 ✓\n",
      "Linea 554/30 ✓\n",
      "Linea 555/30 ✓\n",
      "Linea 556/30 ✓\n",
      "Linea 557/30 ✓\n",
      "Linea 558/30 ✓\n",
      "Linea 559/30 ✓\n",
      "Linea 560/30 ✓\n",
      "Linea 561/30 ✓\n",
      "Linea 562/30 ✓\n",
      "Linea 563/30 ✓\n",
      "Linea 564/30 ✓\n",
      "Linea 565/30 ✓\n",
      "Error procesando linea 566: Extra data: line 11 column 1 (char 305)\n",
      "Linea 567/30 ✓\n",
      "Linea 568/30 ✓\n",
      "Linea 569/30 ✓\n",
      "Linea 570/30 ✓\n",
      "Linea 571/30 ✓\n",
      "Linea 572/30 ✓\n",
      "Linea 573/30 ✓\n",
      "Linea 574/30 ✓\n",
      "Linea 575/30 ✓\n",
      "Linea 576/30 ✓\n",
      "Linea 577/30 ✓\n",
      "Linea 578/30 ✓\n",
      "Linea 579/30 ✓\n",
      "Linea 580/30 ✓\n",
      "Linea 581/30 ✓\n",
      "Linea 582/30 ✓\n",
      "Linea 583/30 ✓\n",
      "Linea 584/30 ✓\n",
      "Linea 585/30 ✓\n",
      "Linea 586/30 ✓\n",
      "Linea 587/30 ✓\n",
      "Linea 588/30 ✓\n",
      "Linea 589/30 ✓\n",
      "Linea 590/30 ✓\n",
      "Linea 591/30 ✓\n",
      "Linea 592/30 ✓\n",
      "Linea 593/30 ✓\n"
     ]
    }
   ],
   "source": [
    "# Trial data\n",
    "\n",
    "import backoff \n",
    "\n",
    "spanish_input_file = \"/home/spoturno/coding/MLSP_Data/Data/Test/Spanish/multilex_test_es_ls_unlabelled.tsv\" # change this\n",
    "output_file = \"/home/spoturno/coding/MLSP/predictions/groq_models_with_crea_corpus.tsv\" # change this\n",
    "\n",
    "sentences = []\n",
    "target_words = []\n",
    "\n",
    "cant_ejemplos = 30\n",
    "i = 0\n",
    "\n",
    "with open(spanish_input_file, 'r', encoding='utf-8') as file, open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    for line in file:\n",
    "        i += 1\n",
    "\n",
    "        # Separamos la linea en tabulaciones \n",
    "        parts = line.strip().split('\\t')\n",
    "        sentence = parts[0]\n",
    "        target_word = parts[1]\n",
    "\n",
    "        # Guardo la oracion y palabra ordenadamente\n",
    "        sentences.append(sentence)\n",
    "        target_words.append(target_word)\n",
    "\n",
    "        try:\n",
    "            # Obtener prediccion\n",
    "            predicciones = call_api_with_backoff(sentence, target_word)\n",
    "            predictions = predicciones['Palabra'].to_list()\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando linea {i}: {e}\")\n",
    "            continue  # Skipear linea y avanzar si ocurrio error\n",
    "\n",
    "        # Preparar la linea a escribir en el archivo de salida\n",
    "        output_line = f\"{sentence}\\t{target_word}\\t\" + '\\t'.join(predictions) + '\\n'\n",
    "        out_file.write(output_line)\n",
    "\n",
    "        print(f\"Linea {i}/{cant_ejemplos} \\u2713\")\n",
    "\n",
    "    if i == 30: print(\"Completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file = \"/home/spoturno/coding/MLSP_Data/Data/Test/Spanish/multilex_test_es_ls_unlabelled.tsv\"\n",
    "results_file = \"/home/spoturno/coding/MLSP/predictions/groq_models_with_crea_corpus_results.tsv\"\n",
    "predictions_file = \"/home/spoturno/coding/MLSP/predictions/groq_models_with_crea_corpus.tsv\"\n",
    "\n",
    "\n",
    "predicciones = []\n",
    "\n",
    "with open(results_file, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file, start=1):\n",
    "        parts = line.strip().split('\\t')\n",
    "        line_predictions = str.join('\\t', parts[2:])\n",
    "        predicciones.append(line_predictions)\n",
    "\n",
    "sentences_and_words = []\n",
    "\n",
    "with open(predictions_file, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file, start=1):\n",
    "        if line == 'ERRORLINE\\n':\n",
    "            sentences_and_words.append('ERRORLINE\\n')\n",
    "        else:\n",
    "            parts = line.strip().split('\\t')\n",
    "            sentence = parts[0]\n",
    "            target_word = parts[1]\n",
    "            sentence_and_word = f\"{sentence}\\t{target_word}\"\n",
    "            sentences_and_words.append(sentence_and_word)\n",
    "\n",
    "\n",
    "with open(predictions_file, 'w', encoding='utf-8') as file:\n",
    "    for line in sentences_and_words:\n",
    "        if line == 'ERRORLINE\\n':\n",
    "            file.write('ERRORLINE\\n')\n",
    "        else:\n",
    "            file.write(line + '\\t' + predicciones.pop(0) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line_number, sentence, target_word):\n",
    "    try:\n",
    "        response = call_api_with_backoff(sentence, target_word)\n",
    "        predictions = response['Palabra'].to_list()\n",
    "        return f\"{sentence}\\t{target_word}\\t\" + '\\t'.join(predictions) + '\\n'\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line {line_number}: {e}\")\n",
    "        return f'ERRORLINE\\n'\n",
    "\n",
    "input_file =  \"/home/spoturno/coding/MLSP_Data/Data/Test/Spanish/multilex_test_es_ls_unlabelled.tsv\"\n",
    "predictions_file = \"/home/spoturno/coding/MLSP/predictions/groq_models_with_crea_corpus.tsv\"\n",
    "\n",
    "lines_to_correct = [18, 46, 51, 55, 102, 135, 145, 182, 183, 230, 231, 245, 263, 279, 304, 312, 315, 330, 342, 345, 358, 377, 382, 430, 439, 440, 509, 537, 540, 566]\n",
    "\n",
    "original_lines = []\n",
    "with open(predictions_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        original_lines.append(line)\n",
    "\n",
    "corrected_lines = []\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file, start=1):\n",
    "        if i in lines_to_correct:\n",
    "            parts = line.strip().split('\\t')\n",
    "            new_line = process_line(i, parts[0], parts[1])\n",
    "            corrected_lines.append(new_line)\n",
    "\n",
    "\n",
    "original_lines[54] = corrected_lines[0]\n",
    "\n",
    "with open(predictions_file, 'w', encoding='utf-8') as file:\n",
    "    for line in original_lines:\n",
    "        if line == 'ERRORLINE\\n' and corrected_lines:\n",
    "            file.write(corrected_lines.pop(0))\n",
    "        else:\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_line(line_number, sentence, target_word):\n",
    "    try:\n",
    "        response = call_api_with_backoff(sentence, target_word)\n",
    "        predictions = response['Palabra'].to_list()\n",
    "        return f\"{sentence}\\t{target_word}\\t\" + '\\t'.join(predictions) + '\\n'\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line {line_number}: {e}\")\n",
    "        return f'ERRORLINE\\n'\n",
    "\n",
    "input_file =  \"/home/spoturno/coding/MLSP_Data/Data/Test/Spanish/multilex_test_es_ls_unlabelled.tsv\"\n",
    "predictions_file = \"/home/spoturno/coding/MLSP/predictions/groq_models_with_crea_corpus.tsv\"\n",
    "\n",
    "lines_to_correct = [18, 46, 51, 55, 102, 135, 145, 182, 183, 230, 231, 245, 263, 279, 304, 312, 315, 330, 342, 345, 358, 377, 382, 430, 439, 440, 509, 537, 540, 566]\n",
    "\n",
    "new_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_to_predict = 245\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file, start=1):\n",
    "        if i == line_to_predict:\n",
    "            parts = line.strip().split('\\t')\n",
    "            sentence = parts[0]\n",
    "            target_word = parts[1]\n",
    "            new_lines.append(process_line(i, sentence, target_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La última columna indica la diferencia favorable entre los flujos de ingresos por exportaciones menos los flujos de egresos por el pago de importaciones.\\tfavorable\\tmejor\\tpositiva\\tfavorable\\tbeneficiosa\\tventajosa\\tprovechosa\\talentadora\\n',\n",
       " 'Si existe igualdad fiscal todos los ciudadanos tendrán constante necesidad de reunirse y de obrar en asuntos que a todos interesa y eso basta para conservar un nexo entre ellos.\\tconservar\\tmantener\\tasegurar\\tproteger\\tconservar\\tpreservar\\tsalvaguardar\\n']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete this if necesary, LLAMA2 is works badly with JSON response output\n",
    "\n",
    "def obtener_predicciones_v2(oracion_p: str, palabra_p: str):\n",
    "    candidatos_mixtral = get_groq_simplifications(MIXTRAL, oracion_p, palabra_p)\n",
    "    df_filtrado_mixtral = obtener_frecuencias_de_sustitutos(candidatos_mixtral)\n",
    "\n",
    "    candidatos_gemma = get_groq_simplifications(GEMMA, oracion_p, palabra_p)\n",
    "    df_filtrado_gemma = obtener_frecuencias_de_sustitutos(candidatos_gemma)\n",
    "\n",
    "    # combinamos filtrados de los modelos\n",
    "    df_merged = pd.concat([df_filtrado_mixtral, df_filtrado_gemma], ignore_index=True)\n",
    "\n",
    "    # Ordenamos segun frecuencia absoluta\n",
    "    df_merged = df_merged.sort_values('Frec.absoluta', ascending=False)\n",
    "\n",
    "    # Elimino duplicados\n",
    "    df_merged = df_merged.drop_duplicates(subset='Palabra')\n",
    "\n",
    "    return df_merged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def obtener_predicciones_v3(oracion_p: str, palabra_p: str):\n",
    "    candidatos_mixtral = get_groq_simplifications(LLAMA, oracion_p, palabra_p)\n",
    "    df_merged = obtener_frecuencias_de_sustitutos(candidatos_mixtral)\n",
    "\n",
    "    # Ordenamos segun frecuencia absoluta\n",
    "    df_merged = df_merged.sort_values('Frec.absoluta', ascending=False)\n",
    "\n",
    "    # Elimino duplicados\n",
    "    df_merged = df_merged.drop_duplicates(subset='Palabra')\n",
    "\n",
    "    return df_merged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
